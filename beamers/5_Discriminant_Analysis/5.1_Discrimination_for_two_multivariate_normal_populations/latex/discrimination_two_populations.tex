\documentclass[aspectratio=169]{beamer}
\usetheme{metropolis}
\usecolortheme{default}
\usefonttheme{default}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{caption}[numbered]

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{verbatim}

\title[Two-Population Discrimination]{MA2003B - Application of Multivariate Methods in Data Science}
\subtitle{Topic 5.1: Discrimination for Two Multivariate Normal Populations}
\author{Dr. Juliho Castillo}
\institute{Tec de Monterrey}
\date{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}{What is Discriminant Analysis?}
  \begin{itemize}
    \item A classification method for distinguishing between two or more groups.
    \item Based on observed characteristics (features) of objects or individuals.
    \item \textbf{Goal:} Assign new observations to the correct population.
    \item Uses statistical models to find optimal decision boundaries.
  \end{itemize}
\end{frame}

\begin{frame}{Two-Population Problem}
  \begin{itemize}
    \item We have two populations: $\pi_0$ and $\pi_1$
    \item Both follow multivariate normal distributions with the same covariance matrix
    \item Different mean vectors: $\boldsymbol{\mu}_0$ and $\boldsymbol{\mu}_1$
    \item \textbf{Question:} Given observation $\mathbf{x}$, which population does it belong to?
  \end{itemize}
\end{frame}

\section{The Model}
\begin{frame}{Mathematical Setup}
  Consider two populations:
  \begin{align}
  \pi_0: \mathbf{x} &\sim \mathcal{N}(\boldsymbol{\mu}_0, \boldsymbol{\Sigma}) \\
  \pi_1: \mathbf{x} &\sim \mathcal{N}(\boldsymbol{\mu}_1, \boldsymbol{\Sigma})
  \end{align}
  \begin{itemize}
    \item Same covariance matrix $\boldsymbol{\Sigma}$ (homoscedasticity assumption)
    \item Different means $\boldsymbol{\mu}_0 \neq \boldsymbol{\mu}_1$
    \item This leads to Linear Discriminant Analysis (LDA)
  \end{itemize}
\end{frame}

\begin{frame}{Optimal Classification Rule}
  \begin{itemize}
    \item Bayes classifier minimizes probability of misclassification
    \item For equal priors and costs, classify $\mathbf{x}$ to $\pi_1$ if:
  \end{itemize}
  $$\frac{f_1(\mathbf{x})}{f_0(\mathbf{x})} > 1$$
  \begin{itemize}
    \item $f_i(\mathbf{x})$ is the probability density function of population $\pi_i$
    \item This is the likelihood ratio test
  \end{itemize}
\end{frame}

\section{Linear Discriminant Function}
\begin{frame}{Deriving the Discriminant Function}
  After algebraic manipulation of the log-likelihood ratio:
  $$\mathbf{w}^T \mathbf{x} + b > 0 \Rightarrow \text{classify as } \pi_1$$
  
  where:
  \begin{align}
  \mathbf{w} &= \boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0) \\
  b &= -\frac{1}{2}(\boldsymbol{\mu}_1^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_1 - \boldsymbol{\mu}_0^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_0)
  \end{align}
\end{frame}

\begin{frame}{Sample Estimates}
  In practice, we estimate parameters from training data:
  \begin{align}
  \hat{\boldsymbol{\mu}}_0 &= \frac{1}{n_0} \sum_{i: y_i = 0} \mathbf{x}_i \\
  \hat{\boldsymbol{\mu}}_1 &= \frac{1}{n_1} \sum_{i: y_i = 1} \mathbf{x}_i \\
  \hat{\boldsymbol{\Sigma}} &= \frac{(n_0-1)\mathbf{S}_0 + (n_1-1)\mathbf{S}_1}{n_0 + n_1 - 2}
  \end{align}
  \begin{itemize}
    \item $\hat{\boldsymbol{\Sigma}}$ is the pooled covariance matrix
  \end{itemize}
\end{frame}

\section{Geometric Interpretation}
\begin{frame}{Decision Boundary}
  \begin{itemize}
    \item Decision boundary: hyperplane where $\mathbf{w}^T \mathbf{x} + b = 0$
    \item Perpendicular to vector $\mathbf{w} = \boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0)$
    \item Separates the two populations optimally
    \item Linear boundary (hence "Linear" Discriminant Analysis)
  \end{itemize}
\end{frame}

\begin{frame}{Geometric Understanding}
  \begin{itemize}
    \item $\mathbf{w}$ points in direction of maximum separation
    \item Projection $\mathbf{w}^T \mathbf{x}$ reduces problem to one dimension
    \item Classification based on which side of threshold the projection falls
    \item Optimal under normality and equal covariance assumptions
  \end{itemize}
\end{frame}

\section{Assumptions}
\begin{frame}{Key Assumptions}
  \begin{itemize}
    \item Both populations follow multivariate normal distributions
    \item Equal covariance matrices: $\boldsymbol{\Sigma}_0 = \boldsymbol{\Sigma}_1$
    \item Sufficient sample sizes for reliable estimation
    \item Independence of observations
  \end{itemize}
  
  \vspace{0.5cm}
  \textbf{Note:} Violations may require Quadratic Discriminant Analysis (QDA) or other methods.
\end{frame}

\section{Python Practice}
\begin{frame}{Python Practice: Two-Population Discrimination}
  \begin{itemize}
    \item Try the script: \texttt{discrimination\_two\_populations\_practice.py}
    \item Demonstrates simulation of two Gaussian populations
    \item Computes LDA coefficients using pooled covariance
    \item Reports classification accuracy
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Running the Script}
  \begin{itemize}
    \item Install required libraries if needed:
  \end{itemize}
  \vspace{0.5cm}
  \begin{verbatim}
pip install numpy
  \end{verbatim}
  \vspace{0.5cm}
  \begin{itemize}
    \item Run the script:
  \end{itemize}
  \vspace{0.5cm}
  \begin{verbatim}
python discrimination_two_populations_practice.py
  \end{verbatim}
\end{frame}

\begin{frame}
  \centering
  \Huge Thank You!
  \vspace{1cm}
  \normalsize Questions?
\end{frame}

\end{document}
