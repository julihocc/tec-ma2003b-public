\documentclass{beamer}

\usetheme{Madrid} % A clean and professional theme

\title{Factor Methods in Multivariate Statistics}
\author{Based on Daniel Zelterman's \\ \textit{Applied Multivariate Statistics with R}}
\date{\today}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx} % Required for including images

% Custom commands for consistent styling
\newcommand{\R}{\texttt{R}}
\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\frame{\titlepage}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{Introduction to Factor Methods}

\begin{frame}
    \frametitle{What are Factor Methods?}
    \begin{itemize}
        \item Deal with data where multiple variables are collected.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Why use Factor Methods?}
    \begin{itemize}
        \item Goal: \alert{Data reduction} or \alert{structural simplification}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Latent Structures}
    \begin{itemize}
        \item Identify underlying (latent) structures within complex multivariate datasets.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{When to apply them}
    \begin{itemize}
        \item Useful when variables are highly correlated, suggesting they measure common unobserved concepts.
    \end{itemize}
\end{frame}

\section{Principal Component Analysis}

\begin{frame}
    \frametitle{PCA: Concept}
    \begin{itemize}
        \item A technique to transform a large set of correlated variables into a smaller set of uncorrelated variables called \alert{principal components}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Components}
    \begin{itemize}
        \item Each principal component is a linear combination of the original variables.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Variance}
    \begin{itemize}
        \item The first principal component accounts for the largest possible variance in the data.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Orthogonality}
    \begin{itemize}
        \item Subsequent components account for decreasing amounts of variance and are \alert{orthogonal} to the preceding components.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Goal}
    \begin{itemize}
        \item Goal: To capture as much variance as possible with as few components as possible, thereby reducing dimensionality.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Mathematical Basis}
    \begin{itemize}
        \item Involves finding the \alert{eigenvalues} and \alert{eigenvectors} of the covariance (or correlation) matrix $\mathbf{S}$ of the data.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Eigen-quantities}
    \begin{itemize}
        \item Eigenvalues ($\lambda_i$) represent the variance explained by each principal component.
        \item Eigenvectors define the directions (loadings) of the principal components in the original variable space.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Loadings}
    \begin{itemize}
        \item Loadings: Coefficients $w_{ij}$ indicating the contribution of each original variable to a principal component. Largest absolute loadings are most important for interpretation.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Mathematical formulation}
    \begin{itemize}
        \item Let $\mathbf{X}$ be the $n\times p$ data matrix (centered). The sample covariance matrix is $\mathbf{S}=\frac{1}{n-1}\mathbf{X}^\top\mathbf{X}$.
        \item Principal components are found by solving the eigenproblem
        \[\mathbf{S}\mathbf{v}_i=\lambda_i\mathbf{v}_i,\qquad i=1,\dots,p,
        \]
        where $\lambda_i$ are eigenvalues and $\mathbf{v}_i$ eigenvectors (loadings).
        \item The $k$-dimensional principal score for observation $\mathbf{x}$ is
        \[\mathbf{z}=\mathbf{V}_k^\top(\mathbf{x}-\bar{\mathbf{x}}),\]
        where $\mathbf{V}_k=[\mathbf{v}_1,\dots,\mathbf{v}_k]$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Variance explained (formula)}
    \begin{itemize}
        \item Total variance: $\mathrm{tr}(\mathbf{S})=\sum_{i=1}^p\lambda_i$.
        \item Proportion explained by component $i$:
        \[\mathrm{Prop}_i=\frac{\lambda_i}{\sum_{j=1}^p\lambda_j}.\]
        \item Cumulative proportion for first $k$ components: $\sum_{i=1}^k\mathrm{Prop}_i$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA: Variance Explained}
    \begin{itemize}
        \item The proportion of total variance accounted for by each component (eigenvalue / sum of all eigenvalues).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scree Plot}
    \begin{itemize}
        \item Plots ordered eigenvalues ($\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_p$) to visually assess the number of important components. Look for an "elbow" in the plot.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Biplot}
    \begin{itemize}
        \item Displays both observations and variable loadings in a low-dimensional space (usually the first two principal components). Arrows indicate variable directions; points represent observations.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{R: PCA Implementation}
    \begin{itemize}
        \item Function: \code{princomp()}
        \item Output: \code{summary()} for variance explained, \texttt{pc\_loadings} (or \code{pc\$loadings}) for eigenvectors, \code{screeplot()}, \code{biplot()}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA Example: Investments}
    \begin{itemize}
        \item Reduce 8 investment categories to fewer components. First PC might capture difference between US stocks and alternative investments.
    \end{itemize}
\end{frame}

\section{Example 1: Investment Allocations}

\begin{frame}
    \frametitle{PCA Example: Kuiper Belt}
    \begin{itemize}
        \item Characterize Kuiper Belt objects (e.g., Pluto's reclassification) via lower-dimensional summaries.
    \end{itemize}
\end{frame}

\section{Example 2: Kuiper Belt Objects}

\begin{frame}
    \frametitle{PCA Example: Health Outcomes}
    \begin{itemize}
        \item Simplify multiple health metrics in hospitals to a few interpretable components.
    \end{itemize}
\end{frame}

\section{Example 3: Health Outcomes in US Hospitals}

\begin{frame}
    \frametitle{PCA: Takeaway}
    \begin{itemize}
        \item PCA finds simpler, interpretable dimensions in complex datasets, providing insights into underlying structures.
    \end{itemize}
\end{frame}

\section{Factor Analysis}

\begin{frame}
    \frametitle{Factor Analysis: Key idea}
    \begin{itemize}
        \item Factor Analysis explicitly posits \alert{latent variables} (factors) that cause the observed correlations among variables.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{FA vs PCA}
    \begin{itemize}
        \item PCA is a data transformation technique; Factor Analysis is a statistical model that explains observed variance using common factors and unique variances.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{FA: Concept}
    \begin{itemize}
        \item Each observed variable is a linear combination of one or more common factors (shared variance) and a unique factor (specific variance and error).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Factor Analysis: Model equations}
    \begin{itemize}
        \item Observed-vector model (common factor model):
        \[\mathbf{x}=\boldsymbol{\mu}+\mathbf{\Lambda}\mathbf{f}+\boldsymbol{\epsilon},\]
        where $\mathbf{x}$ is $p\times 1$, $\mathbf{\Lambda}$ is the $p\times m$ loading matrix, $\mathbf{f}$ is $m\times1$ latent factors, and $\boldsymbol{\epsilon}$ is unique/error terms.
        \item Covariance decomposition:
        \[\boldsymbol{\Sigma}=\mathrm{Cov}(\mathbf{x})=\mathbf{\Lambda}\,\mathrm{Cov}(\mathbf{f})\,\mathbf{\Lambda}^\top+\boldsymbol{\Psi},\]
        with $\boldsymbol{\Psi}$ diagonal (unique variances).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{FA: Identification and uniqueness}
    \begin{itemize}
        \item Common choices: assume $\mathrm{Cov}(\mathbf{f})=\mathbf{I}_m$ (orthonormal factors) and estimate $\mathbf{\Lambda}$ and $\boldsymbol{\Psi}$.
        \item The uniqueness for variable $j$ is $\psi_{jj}$; communalities are $h_j^2=\sum_{i=1}^m\lambda_{ji}^2$ when factors are orthonormal.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{FA: Goal}
    \begin{itemize}
        \item Goal: Identify these latent factors and quantify their influence on the observed variables.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Factor Rotation}
    \begin{itemize}
        \item A crucial step to improve interpretability of factor loadings. Rotations (e.g., Varimax, Promax) aim to achieve "simple structure" where each variable loads highly on only one factor and near zero on others.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{FA: R Implementation}
    \begin{itemize}
        \item Function: \code{factanal()}
        \item Requires specifying the number of factors.
        \item Example: Hamburger nutritional data (Calories, Fat, Sodium, etc.) might reveal factors like "Meat Content" and "Added Ingredients".
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{FA: Interpretation}
    \begin{itemize}
        \item Interpretation often involves a blend of statistical output and domain-specific knowledge.
    \end{itemize}
\end{frame}

\section{Confirmatory Factor Analysis}

\begin{frame}
    \frametitle{Confirmatory Factor Analysis (CFA)}
    \framesubtitle{Supervised Approach and Model Specification}
    \begin{itemize}
        \item In contrast to PCA and exploratory Factor Analysis, CFA is a \alert{supervised} method.
        \item Allows researchers to \alert{test a pre-specified theory} or hypothesis about the factor structure.
        \item Instead of letting the data determine the factors, we hypothesize which variables load onto which latent factors based on prior knowledge.
    \item \textbf{Model Specification}: Defined using text syntax, where \texttt{latent\_variable =~ observed\_variable1 + observed\_variable2}. This indicates that the latent variable is measured by the specified observed variables.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{CFA: Measurement model (matrix form)}
    \begin{itemize}
        \item Measurement model (same as FA) but with a pre-specified zero pattern in $\mathbf{\Lambda}$:
        \[\mathbf{x}=\boldsymbol{\mu}+\mathbf{\Lambda}\mathbf{f}+\boldsymbol{\epsilon}.
        \]
        \item In CFA the researcher fixes which $\lambda_{ji}=0$ (not estimated) and tests model fit (e.g., $\chi^2$, RMSEA, CFI, TLI).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Confirmatory Factor Analysis (CFA)}
    \framesubtitle{\R Implementation and Example}
    \begin{itemize}
        \item \textbf{\R Implementation}:
            \begin{itemize}
                \item Package: \code{lavaan}
                \item Function: \code{cfa()}
                \item Output includes factor loadings, standard errors, and fit measures ($\chi^2$ statistic, p-value).
            \end{itemize}
        \item \textbf{Example: Hamburger Data}
            \begin{itemize}
                \item Hypothesized latent variables: \code{meat} (measured by CalFat, Cal, Fat, SatFat, Protein) and \code{added} (measured by Sodium, Carbs).
                \item This allows incorporating nutritional knowledge directly into the model, leading to more meaningful interpretations.
            \end{itemize}
    \end{itemize}
\end{frame}

\section{Path Analysis}

\begin{frame}
    \frametitle{Path Analysis: Purpose}
    \begin{itemize}
        \item A multivariate statistical technique used to evaluate \alert{causal models} by examining the relationships between observed and latent variables.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Path Analysis: Directed relations}
    \begin{itemize}
        \item Focuses on directed relationships (causal paths) among variables.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Path Analysis: Structural equations}
    \begin{itemize}
        \item A path model is a system of linear equations. Example for three variables:
        \[y_1=\beta_{12}y_2+\beta_{13}y_3+\varepsilon_1,\]
        \[y_2=\beta_{23}y_3+\varepsilon_2,\]
        where $\varepsilon_i$ are disturbance terms and $\beta_{ij}$ are path coefficients.
        \item Path matrices let us write the system compactly: $\mathbf{y}=\mathbf{B}\mathbf{y}+\boldsymbol{\varepsilon}$ and solve $(\mathbf{I}-\mathbf{B})\mathbf{y}=\boldsymbol{\varepsilon}$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Path Analysis: Latent variables}
    \begin{itemize}
        \item Can include both observed and \alert{latent variables} (which are themselves defined by observed indicators).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Path Diagrams}
    \begin{itemize}
        \item Model structure is visually represented by a \alert{path diagram}, showing arrows for directed effects and double-headed arrows for covariances.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Path Analysis: R tools}
    \begin{itemize}
        \item Package: \code{lavaan} (function \code{cfa()} or \code{sem()}). Visualization via \code{semPlot::semPaths()}.
        \item Model specification uses \texttt{=\~} for latent variables and \texttt{~} for regression relationships.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Path Example: Car data}
    \begin{itemize}
        \item Latent variables: \texttt{engine} (hp, cyl, disp, carb) and \texttt{drive\_train} (gear, drat, am).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Path Example: Predictions}
    \begin{itemize}
        \item These latent variables predict empirical measures like miles per gallon (\texttt{mpg}), weight (\texttt{wt}), and quarter-mile time (\texttt{qsec}).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Path Example: Visualisation}
    \begin{itemize}
        \item Path diagrams offer a clear way to convey complex structural relationships.
    \end{itemize}
\end{frame}

\section{Structural Equations and Latent Growth Modeling}
\begin{frame}
    \frametitle{SEM: Overview}
    \begin{itemize}
        \item SEM combines and extends factor analysis and path analysis.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{SEM: Compact notation}
    \begin{itemize}
        \item Measurement model: $\mathbf{x}=\mathbf{\Lambda}_x\mathbf{\xi}+\boldsymbol{\delta}$, $\mathbf{y}=\mathbf{\Lambda}_y\mathbf{\eta}+\boldsymbol{\varepsilon}$.
        \item Structural model: $\mathbf{\eta}=\mathbf{B}\mathbf{\eta}+\mathbf{\Gamma}\mathbf{\xi}+\boldsymbol{\zeta}$.
        \item Combine and estimate parameters using covariance-implied fitting: minimize $\|\mathbf{S}-\boldsymbol{\Sigma}(\theta)\|$ under chosen loss (ML, GLS).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Latent Growth Model (LGM)}
    \begin{itemize}
        \item Individual trajectory model for repeated measures $y_{it}$ (time $t$):
        \[y_{it}=\alpha_i+\beta_i t+\epsilon_{it},\]
        where $\alpha_i$ (intercept) and $\beta_i$ (slope) are latent variables with their own distribution across individuals.
        \item In matrix form, the growth factors are estimated as part of an SEM.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{SEM: Measurement \& Structural}
    \begin{itemize}
        \item Simultaneously estimate measurement models (how latent variables are measured) and structural models (relationships between variables).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{SEM: Features}
    \begin{itemize}
        \item Can model complex relationships: indirect effects, mediation, moderation.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{SEM: R Implementation}
    \begin{itemize}
        \item \textbf{R Implementation}: \code{lavaan} package, using \code{sem()} function.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{LGM: Purpose}
    \begin{itemize}
        \item Latent Growth Models analyze longitudinal data by modeling individual trajectories (intercept and slope latent variables).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{LGM: Applications}
    \begin{itemize}
        \item Example: Modeling home price index trends across cities; latent slopes/intercepts can be influenced by demographics.
    \end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}
    \frametitle{Key Takeaway 1}
    \begin{itemize}
        \item Factor methods (PCA, Factor Analysis, CFA, Path Analysis, SEM/LGM) are essential tools for understanding complex multivariate data.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Takeaway 2}
    \begin{itemize}
        \item They help in \alert{data reduction}, uncovering \alert{latent structures}, and \alert{testing theoretical models}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Takeaway 3}
    \begin{itemize}
        \item The choice of method depends on the research question and the extent of prior knowledge (exploratory vs. confirmatory).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Tools}
    \begin{itemize}
        \item \code{R} packages like \code{stats}, \code{MVA}, \code{lavaan}, and \code{semPlot} provide robust functionalities for implementing these methods.
    \end{itemize}
\end{frame}
\section{Exercises}

\begin{frame}
    \frametitle{Exercises}
    \begin{enumerate}
        \item Apply PCA to the investment allocation dataset: compute PCs, plot a scree, and interpret the first two components.
        \item Repeat PCA for the Kuiper Belt objects dataset and discuss which physical attributes group together.
        \item Use PCA on hospital health outcomes data to derive a compact health-index and compare facility rankings.
        \item Fit an exploratory Factor Analysis on a psychology-style dataset, experiment with rotations (Varimax, Promax) and interpret factors.
        \item Specify and fit a CFA using \code{lavaan} for a hypothesized measurement model; report fit statistics and modifications.
        \item Build a simple path model (or SEM) combining measurement and structural parts; visualize with \code{semPlot}.
    \end{enumerate}
\end{frame}

\section*{References}
\begin{frame}
    \frametitle{References}
    \begin{itemize}
        \item Zelterman, D. (2022). \textit{Applied Multivariate Statistics with R} (Second ed.). Springer.
    \end{itemize}
\end{frame}

\end{document}
