% Comprehensive Factor Analysis Notes - MA2003B Course
\documentclass[a4paper]{tufte-book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
% Note: natbib is already loaded by tufte-book class

% Load shared MA2003B color definitions
\usepackage{../../themes/ma2003b-colors}

% Enhanced color commands for consistent styling
\newcommand{\sectioncolor}[1]{\textcolor{primary}{#1}}

% Section formatting with brand colors
\AtBeginDocument{
  \titleformat{\section}{\Large\bfseries\color{primary}}{\thesection}{1em}{}
  \titleformat{\subsection}{\large\bfseries\color{secondary}}{\thesubsection}{1em}{}
}

% Modern colored environments using tcolorbox
\newtcolorbox{pedagogicalnote}[1][]{
  colback=primarybg,
  colframe=primary,
  coltitle=white,
  colbacktitle=primary,
  fonttitle=\bfseries,
  title=Pedagogical Note,
  rounded corners,
  boxrule=1pt,
  #1
}

\newtcolorbox{learningtip}[1][]{
  colback=successbg,
  colframe=success,
  coltitle=white,
  colbacktitle=success,
  fonttitle=\bfseries,
  title=Learning Tip,
  rounded corners,
  boxrule=1pt,
  #1
}

\newtcolorbox{commonmistake}[1][]{
  colback=dangerbg,
  colframe=danger,
  coltitle=white,
  colbacktitle=danger,
  fonttitle=\bfseries,
  title=Common Mistake,
  rounded corners,
  boxrule=1pt,
  #1
}

\newtcolorbox{mathconcept}[1][]{
  colback=infobg,
  colframe=info,
  coltitle=white,
  colbacktitle=info,
  fonttitle=\bfseries,
  title=Mathematical Concept,
  rounded corners,
  boxrule=1pt,
  #1
}

\newtcolorbox{practicalapplication}[1][]{
  colback=warningbg,
  colframe=warning,
  coltitle=black,
  colbacktitle=warning,
  fonttitle=\bfseries,
  title=Practical Application,
  rounded corners,
  boxrule=1pt,
  #1
}

% Title page content
\title{Factor Analysis: Comprehensive Course Notes}
\author{MA2003B - Application of Multivariate Methods in Data Science}
\date{Tec de Monterrey}

\begin{document}

\frontmatter
\maketitle

% Abstract/Overview
\begin{abstract}
These comprehensive notes cover the complete Factor Analysis chapter of MA2003B, consolidating all six subtopics into a unified pedagogical resource. The material progresses from fundamental objectives through advanced rotation techniques to practical software implementation, providing both theoretical understanding and practical skills for multivariate data analysis.
\end{abstract}

\tableofcontents

\mainmatter

% =============================================================================
% CHAPTER 1: OBJECTIVES OF FACTOR ANALYSIS
% =============================================================================

\chapter{Objectives of Factor Analysis}

\begin{pedagogicalnote}
This opening chapter establishes the conceptual foundation for factor analysis. Students often confuse factor analysis with PCA, so emphasize the fundamental differences early and consistently throughout the course.
\end{pedagogicalnote}

\section{Introduction to Factor Analysis}

Factor Analysis (FA) is a statistical technique used to identify latent variables that explain the correlations among observed variables. Unlike other dimensionality reduction techniques, factor analysis explicitly models measurement error and focuses on identifying underlying constructs that cannot be directly observed.

\marginnote{The term "factor" in factor analysis refers to latent variables or constructs, not to factoring numbers in mathematics.}

\subsection{Historical Development}

Factor analysis was developed in the early 20th century by psychologists seeking to understand human intelligence. Charles Spearman (1904) introduced the concept of a general intelligence factor (g-factor) that could explain correlations among different cognitive tests. This work laid the foundation for modern factor analysis.

\subsection{Fundamental Concept}

The core idea of factor analysis is that observed variables can be expressed as linear combinations of a smaller number of latent factors plus error terms. This allows researchers to:

\begin{enumerate}
\item Reduce the complexity of data while preserving essential information
\item Identify underlying theoretical constructs
\item Understand the structure of relationships among variables
\item Remove measurement error from subsequent analyses
\end{enumerate}

\section{Primary Objectives}

\subsection{Dimensionality Reduction}

Factor analysis reduces the dimensionality of data by expressing $p$ observed variables in terms of $k$ factors, where $k < p$. However, unlike Principal Component Analysis (PCA), factor analysis focuses specifically on the \textit{common variance} among variables.

\marginnote{Common variance is the portion of each variable's variance that is shared with other variables in the analysis.}

\begin{mathconcept}
In factor analysis, each observed variable $X_i$ is decomposed as:
$$X_i = \text{Common Variance} + \text{Unique Variance}$$

The common variance is explained by the factors, while the unique variance includes both specific variance (unique to that variable) and measurement error.
\end{mathconcept}

\subsection{Latent Variable Identification}

Factor analysis seeks to identify latent variables (factors) that represent underlying constructs. These constructs are theoretical entities that cannot be directly measured but influence multiple observed variables.

\begin{practicalapplication}
In personality psychology, researchers cannot directly measure "extraversion" but can observe behaviors like talkativeness, sociability, and assertiveness. Factor analysis can identify extraversion as a latent factor that explains correlations among these behaviors.
\end{practicalapplication}

\subsection{Data Structure Understanding}

By identifying patterns of correlations among variables, factor analysis reveals the underlying structure of data. This helps researchers understand which variables measure similar constructs and how different constructs relate to each other.

\subsection{Measurement Purification}

Factor analysis separates common variance from measurement error and specific variance. This "purification" improves the reliability of measurements and provides cleaner data for subsequent analyses.

\subsection{Theory Development and Testing}

Factor analysis can be used both for theory generation (exploratory factor analysis) and theory testing (confirmatory factor analysis). This dual nature makes it valuable across the research process.

\section{Factor Analysis vs. Principal Component Analysis}

Understanding the differences between Factor Analysis (FA) and Principal Component Analysis (PCA) is crucial for proper application of these techniques.

\begin{table}[h]
\centering
\caption{Comparison of Factor Analysis and Principal Component Analysis}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Aspect} & \textbf{Factor Analysis} & \textbf{Principal Component Analysis} \\
\midrule
Primary Purpose & Identify latent constructs & Data reduction/compression \\
Mathematical Model & $\mathbf{X} = \boldsymbol{\Lambda}\mathbf{F} + \boldsymbol{\varepsilon}$ & $\mathbf{X} = \mathbf{P}\mathbf{Y}$ \\
Variance Analyzed & Common variance only & Total variance \\
Number of Factors & Usually $< p$ & Can equal $p$ \\
Diagonal Values & Communalities ($< 1$) & 1.0 (uses full correlation matrix) \\
Interpretation & Latent psychological constructs & Mathematical transformations \\
Unique Variance & Explicitly modeled & Ignored/absorbed \\
Rotation & Essential for interpretation & Optional \\
Assumptions & Multivariate normality (ML) & Linearity \\
\bottomrule
\end{tabular}
\end{table}

\begin{learningtip}
Use this decision rule: If you want to understand what underlies your variables (theory building/testing), use Factor Analysis. If you want to reduce the number of variables while preserving as much information as possible, use PCA.
\end{learningtip}

\section{Types of Factor Analysis}

\subsection{Exploratory Factor Analysis (EFA)}

EFA is used when researchers have no strong prior theory about the factor structure. The analysis explores the data to identify the number and nature of factors.

\textbf{Characteristics:}
\begin{itemize}
\item No constraints on which variables load on which factors
\item Number of factors determined by data-driven criteria
\item Used for scale development and theory generation
\item More common in early stages of research
\end{itemize}

\subsection{Confirmatory Factor Analysis (CFA)}

CFA is used to test specific hypotheses about factor structure. Researchers specify which variables should load on which factors and test how well this structure fits the data.

\textbf{Characteristics:}
\begin{itemize}
\item Constraints placed on factor loadings based on theory
\item Number of factors specified a priori
\item Uses goodness-of-fit indices to evaluate model adequacy
\item Common in later stages of research and scale validation
\end{itemize}

\section{Applications Across Disciplines}

\subsection{Psychology}

Factor analysis originated in psychology and remains central to the field.

\textbf{Intelligence Research:}
\begin{itemize}
\item General intelligence factor (g-factor)
\item Multiple intelligence theories
\item Cognitive ability batteries
\end{itemize}

\textbf{Personality Assessment:}
\begin{itemize}
\item Big Five personality model
\item Clinical personality inventories
\item Attitude and opinion scales
\end{itemize}

\subsection{Marketing and Business}

\textbf{Market Research:}
\begin{itemize}
\item Customer segmentation based on preferences
\item Brand positioning studies
\item Service quality measurement
\item Product attribute analysis
\end{itemize}

\textbf{Organizational Psychology:}
\begin{itemize}
\item Job satisfaction surveys
\item Leadership assessment
\item Organizational culture measurement
\end{itemize}

\subsection{Finance}

\textbf{Risk Management:}
\begin{itemize}
\item Identifying risk factors in investment portfolios
\item Credit scoring models
\item Economic indicator analysis
\end{itemize}

\subsection{Education}

\textbf{Assessment and Evaluation:}
\begin{itemize}
\item Academic achievement testing
\item Learning style identification
\item Curriculum effectiveness studies
\item Student evaluation instruments
\end{itemize}

\begin{commonmistake}
Students often think factor analysis can be applied to any dataset. Remember that factor analysis is most appropriate when:
\begin{itemize}
\item Variables are correlated (if uncorrelated, no common factors exist)
\item Sample size is adequate (generally $n \geq 5p$ where $p$ is number of variables)
\item Variables are continuous or at least ordinal
\item The goal is to understand underlying constructs, not just reduce variables
\end{itemize}
\end{commonmistake}

% =============================================================================
% CHAPTER 2: FACTOR ANALYSIS EQUATIONS
% =============================================================================

\chapter{Mathematical Foundation of Factor Analysis}

\begin{pedagogicalnote}
This chapter covers the mathematical underpinnings of factor analysis. Students with limited linear algebra background may struggle here, so provide concrete examples alongside abstract concepts.
\end{pedagogicalnote}

\section{The Factor Model}

The fundamental factor analysis model expresses each observed variable as a linear combination of factors plus a unique component:

\begin{mathconcept}
\textbf{Basic Factor Model:}
$$X_i = \lambda_{i1}F_1 + \lambda_{i2}F_2 + \cdots + \lambda_{ik}F_k + \varepsilon_i$$

For all variables simultaneously:
$$\mathbf{X} = \boldsymbol{\Lambda}\mathbf{F} + \boldsymbol{\varepsilon}$$

Where:
\begin{itemize}
\item $\mathbf{X}$: $(n \times p)$ matrix of observed variables
\item $\boldsymbol{\Lambda}$: $(p \times k)$ matrix of factor loadings
\item $\mathbf{F}$: $(n \times k)$ matrix of factor scores
\item $\boldsymbol{\varepsilon}$: $(n \times p)$ matrix of unique factors
\end{itemize}
\end{mathconcept}

\subsection{Assumptions of the Factor Model}

\begin{enumerate}
\item \textbf{Linearity:} Relationships between variables and factors are linear
\item \textbf{Factor Independence:} Common factors are uncorrelated (in orthogonal model)
\item \textbf{Unique Factor Independence:} Unique factors are uncorrelated with each other and with common factors
\item \textbf{Mean Centering:} All variables have mean zero (or are centered)
\end{enumerate}

\section{Factor Loadings}

Factor loadings ($\lambda_{ij}$) represent the correlation between variable $i$ and factor $j$. They are the key to interpreting factors.

\subsection{Properties of Factor Loadings}

\begin{itemize}
\item Range: $-1 \leq \lambda_{ij} \leq 1$
\item Interpretation: Correlation between variable $i$ and factor $j$
\item Sign indicates direction of relationship
\item Magnitude indicates strength of relationship
\end{itemize}

\begin{learningtip}
Interpreting loading magnitudes:
\begin{itemize}
\item $|\lambda| > 0.7$: Strong relationship
\item $0.4 \leq |\lambda| \leq 0.7$: Moderate relationship
\item $|\lambda| < 0.4$: Weak relationship (often ignored in interpretation)
\end{itemize}
\end{learningtip}

\subsection{Loading Matrix Structure}

The loading matrix $\boldsymbol{\Lambda}$ has the following structure:

$$\boldsymbol{\Lambda} = \begin{pmatrix}
\lambda_{11} & \lambda_{12} & \cdots & \lambda_{1k} \\
\lambda_{21} & \lambda_{22} & \cdots & \lambda_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
\lambda_{p1} & \lambda_{p2} & \cdots & \lambda_{pk}
\end{pmatrix}$$

Each row represents a variable's loadings on all factors. Each column represents all variables' loadings on a specific factor.

\section{Communalities and Uniqueness}

\subsection{Communality}

Communality ($h_i^2$) is the proportion of variable $i$'s variance that is explained by the common factors.

\begin{mathconcept}
$$h_i^2 = \sum_{j=1}^{k} \lambda_{ij}^2$$

Properties:
\begin{itemize}
\item $0 \leq h_i^2 \leq 1$
\item Higher values indicate the variable is well-represented by the factors
\item Used to assess adequacy of the factor solution for each variable
\end{itemize}
\end{mathconcept}

\subsection{Uniqueness}

Uniqueness ($u_i^2$) is the proportion of variable $i$'s variance that is not explained by the common factors.

$$u_i^2 = 1 - h_i^2$$

Uniqueness includes:
\begin{itemize}
\item \textbf{Specific variance:} Reliable variance unique to the variable
\item \textbf{Error variance:} Random measurement error
\end{itemize}

\subsection{Variance Decomposition}

For standardized variables, the total variance equals 1:

$$\text{Var}(X_i) = h_i^2 + u_i^2 = 1$$

This fundamental equation shows that each variable's variance is partitioned into common and unique components.

\begin{practicalapplication}
If a depression scale item has $h^2 = 0.64$, then 64\% of its variance is explained by the common depression factor(s), while 36\% is due to factors specific to that item plus measurement error.
\end{practicalapplication}

\section{Correlation Matrix Reproduction}

Factor analysis aims to reproduce the observed correlation matrix using the factor model.

\subsection{Reproduced Correlations}

The correlation between variables $i$ and $j$ can be reproduced as:

$$r_{ij} = \sum_{k=1}^{m} \lambda_{ik}\lambda_{jk}$$

In matrix form:
$$\mathbf{R} = \boldsymbol{\Lambda}\boldsymbol{\Lambda}' + \mathbf{U}^2$$

Where:
\begin{itemize}
\item $\mathbf{R}$: Observed correlation matrix
\item $\boldsymbol{\Lambda}\boldsymbol{\Lambda}'$: Reproduced correlation matrix from common factors
\item $\mathbf{U}^2$: Diagonal matrix of uniqueness values
\end{itemize}

\subsection{Residual Matrix}

The residual matrix contains the differences between observed and reproduced correlations:

$$\mathbf{E} = \mathbf{R} - \boldsymbol{\Lambda}\boldsymbol{\Lambda}'$$

Small residuals indicate good model fit.

\section{Estimation Methods}

\subsection{Principal Axis Factoring (PAF)}

PAF is the most common extraction method for factor analysis.

\begin{algorithm}
\caption{Principal Axis Factoring Algorithm}
\begin{algorithmic}
\STATE 1. Start with correlation matrix $\mathbf{R}$
\STATE 2. Insert initial communality estimates on diagonal
\STATE 3. Extract eigenvalues and eigenvectors
\STATE 4. Compute factor loadings: $\boldsymbol{\Lambda} = \mathbf{V}\sqrt{\boldsymbol{\Delta}}$
\STATE 5. Calculate new communality estimates
\STATE 6. Replace diagonal of $\mathbf{R}$ with new communalities
\STATE 7. Repeat steps 3-6 until convergence
\end{algorithmic}
\end{algorithm}

\textbf{Initial Communality Estimates:}
\begin{itemize}
\item Squared multiple correlation (SMC): Most common
\item Maximum absolute correlation
\item Unity (equivalent to PCA for first iteration)
\end{itemize}

\subsection{Maximum Likelihood Estimation}

Maximum Likelihood (ML) assumes multivariate normality and provides:
\begin{itemize}
\item Parameter estimates
\item Standard errors
\item Goodness-of-fit statistics
\item Significance tests
\end{itemize}

\textbf{ML Objective Function:}
$$F = \text{trace}(\mathbf{S}\boldsymbol{\Sigma}^{-1}) - \log|\mathbf{S}\boldsymbol{\Sigma}^{-1}| - p$$

Where $\mathbf{S}$ is the sample covariance matrix and $\boldsymbol{\Sigma}$ is the model-implied covariance matrix.

\subsection{Other Extraction Methods}

\textbf{Image Factoring:}
\begin{itemize}
\item Based on image analysis concepts
\item Less commonly used
\item Provides anti-image matrices for variable selection
\end{itemize}

\textbf{Alpha Factoring:}
\begin{itemize}
\item Maximizes Cronbach's alpha of factors
\item Good for scale development
\item Less popular than PAF or ML
\end{itemize}

\begin{commonmistake}
Students often confuse extraction methods with rotation methods. Remember:
\begin{itemize}
\item \textbf{Extraction} determines how many factors to retain and their initial form
\item \textbf{Rotation} transforms the factors to improve interpretability
\end{itemize}
These are separate steps in factor analysis!
\end{commonmistake}

% =============================================================================
% CHAPTER 3: DETERMINING THE NUMBER OF FACTORS
% =============================================================================

\chapter{Choosing the Appropriate Number of Factors}

\begin{pedagogicalnote}
Determining the number of factors is often the most crucial decision in factor analysis. Students should learn multiple approaches and understand that no single method is definitive. Encourage triangulation across methods.
\end{pedagogicalnote}

\section{The Factor Retention Problem}

Choosing the optimal number of factors is a critical decision that affects:
\begin{itemize}
\item Interpretability of the solution
\item Parsimony of the model
\item Replicability across samples
\item Theoretical meaningfulness
\end{itemize}

\marginnote{Under-extraction (too few factors) leads to loss of important information. Over-extraction (too many factors) leads to factors that are not replicable.}

\section{Kaiser Criterion (Eigenvalue > 1 Rule)}

The Kaiser criterion retains factors with eigenvalues greater than 1.0.

\subsection{Rationale}

An eigenvalue represents the amount of variance explained by a factor. Since each standardized variable contributes 1 unit of variance, a factor should explain more variance than a single variable to be retained.

\subsection{Implementation}

\begin{enumerate}
\item Compute eigenvalues of the correlation matrix
\item Count eigenvalues $> 1.0$
\item Retain that number of factors
\end{enumerate}

\begin{mathconcept}
For correlation matrix $\mathbf{R}$ with eigenvalues $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p$:

Retain factor $j$ if $\lambda_j > 1.0$

Total variance explained by retained factors: $\sum_{j=1}^{k} \lambda_j$ where $k$ is the number of retained factors.
\end{mathconcept}

\subsection{Advantages and Limitations}

\textbf{Advantages:}
\begin{itemize}
\item Simple and objective
\item Widely available in statistical software
\item Good baseline for comparison with other methods
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
\item Often over-extracts factors, especially with large numbers of variables
\item Doesn't consider sample size
\item Can be affected by the number of variables
\item May retain chance factors
\end{itemize}

\begin{learningtip}
The Kaiser criterion works best with:
\begin{itemize}
\item 20-50 variables
\item Moderate to high communalities (> 0.5)
\item Sample sizes > 250
\end{itemize}
It tends to over-extract with more variables or smaller samples.
\end{learningtip}

\section{Scree Plot Method}

The scree plot method, developed by Cattell (1966), involves plotting eigenvalues in descending order and looking for the "elbow" or break point.

\subsection{Procedure}

\begin{enumerate}
\item Calculate eigenvalues
\item Plot eigenvalues (y-axis) against factor number (x-axis)
\item Identify the point where the slope changes dramatically
\item Retain factors before this "elbow" point
\end{enumerate}

\subsection{Interpretation Guidelines}

\textbf{Clear Elbow:} Easy to identify break point
\begin{itemize}
\item Steep drop followed by gradual decline
\item Retain factors before the break
\end{itemize}

\textbf{Gradual Decline:} No clear break point
\begin{itemize}
\item Look for the last factor before the slope becomes very gradual
\item Consider theoretical meaningfulness
\end{itemize}

\textbf{Multiple Breaks:} More than one possible break point
\begin{itemize}
\item Consider both solutions
\item Use other retention criteria
\item Evaluate theoretical interpretability
\end{itemize}

\subsection{Advantages and Limitations}

\textbf{Advantages:}
\begin{itemize}
\item Visual and intuitive
\item Less affected by number of variables than Kaiser criterion
\item Considers the relative importance of factors
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
\item Subjective interpretation required
\item Can be ambiguous with multiple or no clear breaks
\item Requires experience to interpret correctly
\end{itemize}

\section{Parallel Analysis}

Parallel Analysis, developed by Horn (1965), compares eigenvalues from real data to those from random data with the same dimensions.

\subsection{Procedure}

\begin{algorithm}
\caption{Horn's Parallel Analysis}
\begin{algorithmic}
\STATE 1. Generate random dataset with same $n$ and $p$ as real data
\STATE 2. Compute eigenvalues for random data
\STATE 3. Repeat steps 1-2 many times (typically 1000)
\STATE 4. Calculate mean eigenvalues for each factor across iterations
\STATE 5. Retain factors where real eigenvalues > mean random eigenvalues
\end{algorithmic}
\end{algorithm}

\subsection{Theoretical Foundation}

Random data still produces eigenvalues > 0 due to sampling variation. Only factors that explain more variance than would be expected by chance should be retained.

\begin{mathconcept}
Retain factor $j$ if:
$$\lambda_{j,\text{observed}} > \lambda_{j,\text{random}}$$

Where $\lambda_{j,\text{random}}$ is the mean of the $j$th eigenvalue across random datasets.
\end{mathconcept}

\subsection{Variations}

\textbf{Raw Data Parallel Analysis:}
\begin{itemize}
\item Generates random normal data
\item Most common implementation
\end{itemize}

\textbf{Permutation Parallel Analysis:}
\begin{itemize}
\item Randomly permutes actual data values
\item Preserves distributional characteristics
\end{itemize}

\textbf{Bootstrap Parallel Analysis:}
\begin{itemize}
\item Resamples from actual data
\item Accounts for sampling variability
\end{itemize}

\subsection{Advantages}

\begin{itemize}
\item Accounts for chance relationships
\item Considers sample size and number of variables
\item Generally more accurate than Kaiser criterion
\item Can be implemented with various data types
\end{itemize}

\begin{learningtip}
Parallel analysis is considered the "gold standard" for factor retention by many researchers. When in doubt, trust parallel analysis over other methods, but always consider theoretical meaningfulness.
\end{learningtip}

\section{Goodness-of-Fit Measures}

When using Maximum Likelihood estimation, several fit indices can guide factor retention.

\subsection{Chi-Square Test}

Tests the hypothesis that the $k$-factor model fits the data perfectly.

$$H_0: \boldsymbol{\Sigma} = \boldsymbol{\Lambda}\boldsymbol{\Lambda}' + \boldsymbol{\Psi}$$

Where $\boldsymbol{\Psi}$ is the diagonal matrix of unique variances.

\textbf{Degrees of Freedom:}
$$df = \frac{p(p-1)}{2} - pk + \frac{k(k-1)}{2}$$

\textbf{Interpretation:}
\begin{itemize}
\item Non-significant $\chi^2$: Model fits adequately
\item Significant $\chi^2$: Model fits poorly
\item Sensitive to sample size (almost always significant with large $n$)
\end{itemize}

\subsection{Root Mean Square Error of Approximation (RMSEA)}

$$\text{RMSEA} = \sqrt{\frac{\chi^2 - df}{df(n-1)}}$$

\textbf{Guidelines:}
\begin{itemize}
\item RMSEA $< 0.05$: Good fit
\item RMSEA $< 0.08$: Adequate fit
\item RMSEA $> 0.10$: Poor fit
\end{itemize}

\subsection{Comparative Fit Index (CFI)}

Compares the proposed model to a baseline (independence) model.

$$\text{CFI} = 1 - \frac{\max(\chi^2_{\text{model}} - df_{\text{model}}, 0)}{\max(\chi^2_{\text{baseline}} - df_{\text{baseline}}, 0)}$$

\textbf{Guidelines:}
\begin{itemize}
\item CFI $> 0.95$: Good fit
\item CFI $> 0.90$: Adequate fit
\end{itemize}

\section{Practical Guidelines for Factor Retention}

\subsection{Triangulation Approach}

Use multiple criteria and look for convergent evidence:

\begin{enumerate}
\item Apply Kaiser criterion as baseline
\item Examine scree plot for clear breaks
\item Conduct parallel analysis
\item Consider theoretical meaningfulness
\item Evaluate interpretability of solutions
\end{enumerate}

\subsection{Sample Size Considerations}

\begin{table}[h]
\centering
\caption{Factor Retention Method Performance by Sample Size}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Sample Size} & \textbf{Recommended Methods} & \textbf{Avoid} \\
\midrule
$n < 100$ & Parallel Analysis, Theory & Kaiser Criterion \\
$100 \leq n < 250$ & Parallel Analysis, Scree Plot & Kaiser with many variables \\
$n \geq 250$ & All methods reliable & None specifically \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Number of Variables Considerations}

\begin{itemize}
\item \textbf{Few variables ($p < 20$):} Kaiser criterion may work well
\item \textbf{Moderate variables ($20 \leq p \leq 50$):} Multiple methods recommended
\item \textbf{Many variables ($p > 50$):} Avoid Kaiser criterion, use parallel analysis
\end{itemize}

\begin{commonmistake}
Never rely solely on one factor retention method. Different methods can suggest different numbers of factors, and the "correct" number often depends on your research goals:
\begin{itemize}
\item \textbf{Exploratory research:} May accept more factors to avoid missing important dimensions
\item \textbf{Scale development:} May prefer fewer, well-defined factors
\item \textbf{Theory testing:} Number determined by theoretical considerations
\end{itemize}
\end{commonmistake}

\section{Cross-Validation Approaches}

\subsection{Split-Sample Validation}

\begin{enumerate}
\item Randomly split sample into two halves
\item Determine number of factors using half 1
\item Test this solution on half 2
\item Compare factor structures across halves
\end{enumerate}

\subsection{k-Fold Cross-Validation}

\begin{enumerate}
\item Divide sample into $k$ folds
\item For each fold, fit model on remaining $k-1$ folds
\item Test fit on hold-out fold
\item Average results across all folds
\end{enumerate}

These approaches help ensure that the chosen number of factors is stable and replicable.

% =============================================================================
% CHAPTER 4: FACTOR ROTATION
% =============================================================================

\chapter{Factor Rotation Methods}

\begin{pedagogicalnote}
Factor rotation is often confusing for students because the mathematical transformations seem to change the factors arbitrarily. Emphasize that rotation preserves the essential relationships while improving interpretability. Use geometric analogies when possible.
\end{pedagogicalnote}

\section{The Need for Factor Rotation}

Initial factor solutions from extraction methods are often difficult to interpret because:

\begin{itemize}
\item Variables tend to load moderately on multiple factors
\item Factors lack clear substantive meaning
\item The solution doesn't achieve "simple structure"
\end{itemize}

Factor rotation transforms the initial solution to achieve better interpretability while preserving the essential mathematical relationships.

\marginnote{Think of rotation like changing your viewing angle of a 3D object. The object doesn't change, but different angles reveal different features more clearly.}

\section{Simple Structure}

Thurstone (1947) defined simple structure as the ideal factor solution with these characteristics:

\begin{enumerate}
\item Each variable should have high loadings on few factors
\item Each factor should have high loadings on few variables
\item Each pair of factors should have few variables with high loadings on both
\end{enumerate}

\begin{learningtip}
Simple structure makes factors easier to interpret because:
\begin{itemize}
\item Variables clearly "belong" to specific factors
\item Factors represent distinct constructs
\item Naming factors becomes straightforward
\end{itemize}
\end{learningtip}

\section{Orthogonal Rotation Methods}

Orthogonal rotations maintain the independence (zero correlation) among factors.

\subsection{Varimax Rotation}

Varimax is the most popular orthogonal rotation method. It maximizes the variance of squared loadings within each factor.

\begin{mathconcept}
\textbf{Varimax Criterion:}
$$V = \sum_{j=1}^{k} \left[ \frac{1}{p}\sum_{i=1}^{p} \lambda_{ij}^4 - \left(\frac{1}{p}\sum_{i=1}^{p} \lambda_{ij}^2\right)^2 \right]$$

This criterion is maximized when each factor has a few large loadings and many small loadings.
\end{mathconcept}

\textbf{Algorithm Overview:}
\begin{algorithm}
\caption{Varimax Rotation Algorithm}
\begin{algorithmic}
\STATE 1. Start with unrotated factor matrix $\boldsymbol{\Lambda}_0$
\STATE 2. For each pair of factors $(j,k)$:
\STATE \quad a. Calculate optimal rotation angle $\phi$
\STATE \quad b. Apply rotation: $\boldsymbol{\Lambda}_{new} = \boldsymbol{\Lambda}_{old} \mathbf{T}$
\STATE 3. Check convergence criterion
\STATE 4. If not converged, return to step 2
\STATE 5. Output rotated matrix $\boldsymbol{\Lambda}_r$
\end{algorithmic}
\end{algorithm}

\textbf{Properties:}
\begin{itemize}
\item Tends to produce factors with a few high loadings and many near-zero loadings
\item Good for identifying distinct, non-overlapping factors
\item Most interpretable orthogonal method
\item Available in all statistical software packages
\end{itemize}

\subsection{Quartimax Rotation}

Quartimax maximizes the variance of squared loadings within variables (across factors).

\textbf{Quartimax Criterion:}
$$Q = \sum_{i=1}^{p} \left[ \frac{1}{k}\sum_{j=1}^{k} \lambda_{ij}^4 - \left(\frac{1}{k}\sum_{j=1}^{k} \lambda_{ij}^2\right)^2 \right]$$

\textbf{Properties:}
\begin{itemize}
\item Tends to create a general factor
\item Each variable loads primarily on one factor
\item Less popular than Varimax
\item Can be useful when a general factor is expected
\end{itemize}

\subsection{Equimax Rotation}

Equimax combines the Varimax and Quartimax criteria.

\textbf{Properties:}
\begin{itemize}
\item Balances simplification of factors and variables
\item Compromise between Varimax and Quartimax
\item Rarely used in practice
\end{itemize}

\section{Mathematical Properties of Rotation}

\subsection{Transformation Matrix}

Rotation is accomplished through multiplication by a transformation matrix $\mathbf{T}$:

$$\boldsymbol{\Lambda}_r = \boldsymbol{\Lambda}\mathbf{T}$$

For orthogonal rotation, $\mathbf{T}$ is orthogonal: $\mathbf{T}\mathbf{T}' = \mathbf{I}$

\subsection{Invariant Properties}

The following properties are preserved under orthogonal rotation:

\begin{itemize}
\item \textbf{Communalities:} $h_i^2$ remains unchanged for each variable
\item \textbf{Total variance explained:} Sum of eigenvalues is constant
\item \textbf{Reproduced correlations:} $\boldsymbol{\Lambda}_r\boldsymbol{\Lambda}_r' = \boldsymbol{\Lambda}\boldsymbol{\Lambda}'$
\item \textbf{Unique variances:} $u_i^2$ values remain the same
\end{itemize}

\subsection{Changed Properties}

\begin{itemize}
\item Individual factor loadings
\item Variance explained by each factor
\item Factor interpretation
\end{itemize}

\begin{practicalapplication}
Imagine you're analyzing survey data about job satisfaction. Before rotation, employees might load moderately on all factors. After Varimax rotation, you might find:
\begin{itemize}
\item Factor 1: Pay and benefits items (Compensation)
\item Factor 2: Supervisor and coworker items (Social environment)
\item Factor 3: Task variety and challenge items (Work content)
\end{itemize}
Each factor now has a clear meaning!
\end{practicalapplication}

\section{Choosing Between Orthogonal Methods}

\begin{table}[h]
\centering
\caption{Comparison of Orthogonal Rotation Methods}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Method} & \textbf{Optimizes} & \textbf{Best Used When} & \textbf{Typical Result} \\
\midrule
Varimax & Factor simplicity & Factors expected to be distinct & Clear factor structure \\
Quartimax & Variable simplicity & General factor expected & One dominant factor \\
Equimax & Both & Uncertain about structure & Compromise solution \\
\bottomrule
\end{tabular}
\end{table}

\textbf{General Recommendation:} Start with Varimax rotation unless you have specific theoretical reasons to expect a general factor (in which case, try Quartimax).

\section{Interpreting Rotated Solutions}

\subsection{Loading Interpretation Guidelines}

\begin{table}[h]
\centering
\caption{Factor Loading Interpretation Guidelines}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Loading Magnitude} & \textbf{Interpretation} \\
\midrule
$\geq 0.71$ & Excellent (50\% or more variance explained) \\
$\geq 0.63$ & Very good (40\% variance explained) \\
$\geq 0.55$ & Good (30\% variance explained) \\
$\geq 0.45$ & Fair (20\% variance explained) \\
$\geq 0.32$ & Poor (10\% variance explained) \\
$< 0.32$ & Usually not interpreted \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Factor Naming Process}

\begin{enumerate}
\item \textbf{Identify salient loadings:} Focus on loadings $\geq 0.40$ (or your chosen cutoff)
\item \textbf{Examine item content:} What do the high-loading variables have in common?
\item \textbf{Consider theoretical frameworks:} What construct do these items measure?
\item \textbf{Create descriptive names:} Use clear, concise labels
\item \textbf{Validate interpretations:} Do the names make theoretical sense?
\end{enumerate}

\begin{learningtip}
Good factor names are:
\begin{itemize}
\item \textbf{Descriptive:} Clearly indicate what the factor measures
\item \textbf{Concise:} Usually 1-3 words
\item \textbf{Theoretically grounded:} Connect to existing literature
\item \textbf{Distinctive:} Clearly different from other factors
\end{itemize}

Avoid generic names like "Factor 1" or overly specific names that don't capture the full factor.
\end{learningtip}

\section{Geometric Interpretation of Rotation}

\subsection{Two-Factor Example}

Consider two factors plotted in 2D space with variables as points:

\begin{itemize}
\item \textbf{Before rotation:} Variables cluster around the 45° line
\item \textbf{After rotation:} Axes rotated so variables cluster near the axes
\end{itemize}

The rotation doesn't move the variable points—it rotates the factor axes to positions that better capture the variable clusters.

\subsection{Rotation Angles}

For a two-factor solution, the rotation angle $\phi$ can be calculated to optimize the chosen criterion (e.g., Varimax). The transformation matrix becomes:

$$\mathbf{T} = \begin{pmatrix}
\cos\phi & \sin\phi \\
-\sin\phi & \cos\phi
\end{pmatrix}$$

\section{Limitations of Orthogonal Rotation}

\begin{itemize}
\item \textbf{Unrealistic independence assumption:} Many real-world factors are correlated
\item \textbf{Forced orthogonality:} May distort natural factor structure
\item \textbf{Interpretability vs. realism trade-off:} Simple structure may not reflect reality
\end{itemize}

\begin{commonmistake}
Students often think rotation "changes" the factor analysis results. Emphasize that rotation:
\begin{itemize}
\item Does NOT change the total variance explained
\item Does NOT change communalities
\item Does NOT change the essential relationships between variables
\item DOES improve interpretability by redistributing variance among factors
\end{itemize}

Think of rotation as changing your perspective, not changing the underlying data structure.
\end{commonmistake}

% =============================================================================
% CHAPTER 5: OBLIQUE ROTATION METHODS
% =============================================================================

\chapter{Oblique Rotation Methods}

\begin{pedagogicalnote}
Oblique rotation introduces complexity by allowing factor correlations, but often provides more realistic models. Students need to understand when oblique rotation is appropriate and how to interpret the additional information it provides.
\end{pedagogicalnote}

\section{When to Use Oblique Rotation}

Oblique rotation allows factors to be correlated, which is often more realistic than the orthogonal assumption.

\subsection{Theoretical Considerations}

Consider oblique rotation when:

\begin{itemize}
\item \textbf{Theory suggests factor correlations:} Intelligence factors, personality traits, attitude dimensions often correlate
\item \textbf{Orthogonal rotation produces complex patterns:} Variables load substantially on multiple factors
\item \textbf{Exploratory analysis:} Let data determine the degree of factor correlation
\end{itemize}

\begin{practicalapplication}
In educational research, different academic subjects (math, reading, science) might be correlated because they all involve general cognitive ability. Forcing these factors to be uncorrelated through orthogonal rotation might distort the true relationships.
\end{practicalapplication}

\subsection{Empirical Indicators}

Consider oblique rotation when orthogonal solutions show:
\begin{itemize}
\item Many variables with moderate-to-high loadings on multiple factors
\item Difficulty achieving simple structure
\item Theoretically related constructs forced to be independent
\end{itemize}

\section{Direct Oblimin Rotation}

Direct Oblimin is the most general and flexible oblique rotation method.

\subsection{The Oblimin Criterion}

\begin{mathconcept}
\textbf{Direct Oblimin Criterion:}
$$O = \sum_{j=1}^{k} \sum_{i=1}^{p} \lambda_{ij}^2 \sum_{\ell \neq j} \lambda_{i\ell}^2 - \frac{\delta}{4} \sum_{j=1}^{k} \left(\sum_{i=1}^{p} \lambda_{ij}^2\right)^2$$

Where $\delta$ (delta) is a parameter controlling the degree of obliqueness.
\end{mathconcept}

\subsection{Delta Parameter Values}

\begin{table}[h]
\centering
\caption{Direct Oblimin Delta Parameter Values}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Delta Value} & \textbf{Method Name} & \textbf{Characteristics} \\
\midrule
$\delta = 0$ & Quartimin & Most oblique, allows highest correlations \\
$\delta = 0.5$ & Biquartimin & Moderate obliqueness \\
$\delta = 1$ & Covarimin & Less oblique \\
Default (software-specific) & Often $\delta = 0$ & Varies by program \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Properties of Direct Oblimin}

\begin{itemize}
\item \textbf{Flexibility:} Can produce solutions ranging from nearly orthogonal to highly oblique
\item \textbf{Convergence:} Generally converges to stable solutions
\item \textbf{Interpretability:} Often produces cleaner simple structure than orthogonal methods
\end{itemize}

\section{Promax Rotation}

Promax is a computationally efficient two-step oblique rotation method.

\subsection{Promax Algorithm}

\begin{algorithm}
\caption{Promax Rotation Procedure}
\begin{algorithmic}
\STATE 1. Perform Varimax rotation to get $\boldsymbol{\Lambda}_V$
\STATE 2. Create target matrix by raising loadings to power $k$: $\mathbf{T} = \text{sign}(\boldsymbol{\Lambda}_V) \cdot |\boldsymbol{\Lambda}_V|^k$
\STATE 3. Normalize target matrix
\STATE 4. Find transformation that brings $\boldsymbol{\Lambda}_V$ closest to $\mathbf{T}$
\STATE 5. Apply transformation to get oblique solution
\end{algorithmic}
\end{algorithm}

\subsection{Power Parameter}

The power parameter $k$ (usually $k = 2, 3, \text{ or } 4$) controls:
\begin{itemize}
\item \textbf{Higher $k$:} More oblique solution, simpler structure
\item \textbf{Lower $k$:} More orthogonal solution, more complex structure
\end{itemize}

\subsection{Advantages of Promax}

\begin{itemize}
\item \textbf{Computational efficiency:} Faster than Direct Oblimin
\item \textbf{Stability:} Less prone to convergence problems
\item \textbf{Simplicity:} Fewer parameters to specify
\item \textbf{Interpretability:} Often produces very clean simple structure
\end{itemize}

\section{Pattern Matrix vs. Structure Matrix}

Oblique rotation produces two loading matrices that provide different information.

\subsection{Pattern Matrix ($\mathbf{P}$)}

The pattern matrix contains partial regression coefficients representing the unique relationship between each variable and each factor.

\begin{mathconcept}
\textbf{Pattern Matrix Interpretation:}
\begin{itemize}
\item Values represent unique contributions of factors to variables
\item Similar to partial regression coefficients
\item Used for factor interpretation and naming
\item Values can exceed $\pm 1.0$ in highly correlated factor solutions
\end{itemize}
\end{mathconcept}

\subsection{Structure Matrix ($\mathbf{S}$)}

The structure matrix contains simple correlations between variables and factors.

\begin{mathconcept}
\textbf{Structure Matrix Calculation:}
$$\mathbf{S} = \mathbf{P}\boldsymbol{\Phi}$$

Where:
\begin{itemize}
\item $\mathbf{S}$: Structure matrix
\item $\mathbf{P}$: Pattern matrix  
\item $\boldsymbol{\Phi}$: Factor correlation matrix
\end{itemize}
\end{mathconcept}

\subsection{Which Matrix to Use?}

\begin{table}[h]
\centering
\caption{Pattern vs. Structure Matrix Usage}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Purpose} & \textbf{Use} & \textbf{Rationale} \\
\midrule
Factor interpretation & Pattern matrix & Shows unique relationships \\
Factor naming & Pattern matrix & Cleaner, less confounded \\
Computing factor scores & Pattern matrix & Avoids double-counting \\
Understanding total relationships & Structure matrix & Shows all connections \\
Reporting correlations & Structure matrix & Intuitive correlation interpretation \\
\bottomrule
\end{tabular}
\end{table}

\begin{learningtip}
\textbf{Rule of thumb:} Use the pattern matrix for interpretation and factor naming. The structure matrix is useful for understanding the total relationships but can be confusing because of indirect effects through factor correlations.
\end{learningtip}

\section{Factor Correlation Matrix}

The factor correlation matrix ($\boldsymbol{\Phi}$) shows correlations among factors.

\subsection{Interpreting Factor Correlations}

\begin{table}[h]
\centering
\caption{Factor Correlation Interpretation Guidelines}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Correlation Magnitude} & \textbf{Interpretation} \\
\midrule
$|r| < 0.32$ & Weak correlation; orthogonal rotation acceptable \\
$0.32 \leq |r| < 0.70$ & Moderate correlation; oblique rotation justified \\
$|r| \geq 0.70$ & High correlation; consider fewer factors \\
$|r| > 0.85$ & Very high; factors may not be distinguishable \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Implications of Factor Correlations}

\textbf{Low Correlations ($|r| < 0.32$):}
\begin{itemize}
\item Factors are relatively independent
\item Orthogonal and oblique solutions will be similar
\item Simpler orthogonal solution may be preferred
\end{itemize}

\textbf{Moderate Correlations ($0.32 \leq |r| < 0.70$):}
\begin{itemize}
\item Oblique rotation provides better fit to data
\item Factor correlations are meaningful and interpretable
\item Common in psychological and social research
\end{itemize}

\textbf{High Correlations ($|r| \geq 0.70$):}
\begin{itemize}
\item Question whether factors are truly distinct
\item Consider extracting fewer factors
\item May indicate need for higher-order factor analysis
\end{itemize}

\section{Higher-Order Factor Analysis}

When oblique factors are highly correlated, higher-order factor analysis can identify factors that explain the correlations among first-order factors.

\subsection{Procedure}

\begin{enumerate}
\item Conduct oblique rotation to get first-order factors
\item Factor analyze the factor correlation matrix $\boldsymbol{\Phi}$
\item Identify second-order factors that explain first-order factor correlations
\item Can continue to third-order factors if needed
\end{enumerate}

\subsection{Applications}

\textbf{Intelligence Research:}
\begin{itemize}
\item First-order factors: Verbal, numerical, spatial abilities
\item Second-order factor: General intelligence (g-factor)
\end{itemize}

\textbf{Personality Research:}
\begin{itemize}
\item First-order factors: Specific personality facets
\item Second-order factors: Big Five personality dimensions
\end{itemize}

\section{Comparing Orthogonal and Oblique Solutions}

\subsection{Decision Framework}

\begin{algorithm}
\caption{Choosing Between Orthogonal and Oblique Rotation}
\begin{algorithmic}
\STATE 1. Start with oblique rotation (e.g., Direct Oblimin)
\STATE 2. Examine factor correlation matrix $\boldsymbol{\Phi}$
\STATE 3. IF most $|r| < 0.32$ THEN use orthogonal (Varimax)
\STATE 4. ELSE IF theoretical reasons for independence THEN use orthogonal
\STATE 5. ELSE use oblique solution
\STATE 6. Compare interpretability and theoretical meaningfulness
\end{algorithmic}
\end{algorithm}

\subsection{Reporting Both Solutions}

Some researchers report both orthogonal and oblique solutions:
\begin{itemize}
\item \textbf{Orthogonal:} For simplicity and comparison with literature
\item \textbf{Oblique:} For more realistic modeling of relationships
\end{itemize}

\section{Practical Considerations}

\subsection{Software Implementation}

\textbf{R (psych package):}
```r
fa(data, nfactors=3, rotate="oblimin")
fa(data, nfactors=3, rotate="promax")
```

\textbf{Python (factor_analyzer):}
```python
fa = FactorAnalyzer(n_factors=3, rotation='oblimin')
fa = FactorAnalyzer(n_factors=3, rotation='promax')
```

\textbf{SPSS:}
```
FACTOR /ROTATION=OBLIMIN
FACTOR /ROTATION=PROMAX
```

\subsection{Reporting Standards}

When reporting oblique rotation results, include:
\begin{itemize}
\item Pattern matrix for interpretation
\item Factor correlation matrix
\item Justification for choosing oblique over orthogonal
\item Comparison with orthogonal solution (if space permits)
\end{itemize}

\begin{commonmistake}
Common mistakes with oblique rotation:
\begin{itemize}
\item \textbf{Using structure matrix for interpretation:} Leads to over-interpretation due to indirect effects
\item \textbf{Ignoring factor correlations:} The correlations are part of the solution and should be reported
\item \textbf{Using oblique rotation unnecessarily:} When factors are uncorrelated, orthogonal rotation is simpler
\item \textbf{Not comparing solutions:} Always compare orthogonal and oblique solutions
\end{itemize}
\end{commonmistake}

% =============================================================================
% CHAPTER 6: SOFTWARE IMPLEMENTATION
% =============================================================================

\chapter{Software Implementation and Best Practices}

\begin{pedagogicalnote}
This chapter provides practical guidance for implementing factor analysis across different software platforms. Emphasize that different software may produce slightly different results due to different default settings and algorithms. Students should verify important results across multiple platforms.
\end{pedagogicalnote}

\section{Python Implementation}

Python offers several packages for factor analysis, each with different strengths and capabilities.

\subsection{scikit-learn}

The \texttt{FactorAnalysis} class in scikit-learn provides basic factor analysis functionality.

\textbf{Strengths:}
\begin{itemize}
\item Integration with scikit-learn ecosystem
\item Consistent API with other ML methods
\item Good for basic exploratory analysis
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
\item Limited rotation options (only Varimax)
\item No parallel analysis
\item Minimal diagnostic output
\end{itemize}

\textbf{Example Implementation:}
```python
from sklearn.decomposition import FactorAnalysis
from sklearn.datasets import load_digits
import numpy as np

# Load data
data = load_digits().data

# Standardize
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# Factor analysis
fa = FactorAnalysis(n_components=5, random_state=42)
fa.fit(data_scaled)

# Results
components = fa.components_  # Factor loadings
noise_variance = fa.noise_variance_  # Unique variances
```

\subsection{factor\_analyzer Package}

The \texttt{factor\_analyzer} package provides comprehensive factor analysis capabilities specifically designed for this purpose.

\textbf{Strengths:}
\begin{itemize}
\item Multiple extraction methods (ML, PAF, etc.)
\item Various rotation options (Varimax, Promax, Oblimin)
\item Parallel analysis implementation
\item Comprehensive diagnostic output
\item Better suited for psychometric applications
\end{itemize}

\textbf{Installation:}
```bash
pip install factor_analyzer
```

\textbf{Comprehensive Example:}
```python
import pandas as pd
import numpy as np
from factor_analyzer import FactorAnalyzer, calculate_bartlett_sphericity, calculate_kmo
from factor_analyzer.factor_analyzer import FactorAnalyzer
import matplotlib.pyplot as plt

# Load data (example with built-in dataset)
from factor_analyzer.datasets import load_bank_data
data = load_bank_data()

# Adequacy tests
chi_square, p_value = calculate_bartlett_sphericity(data)
print(f'Bartlett sphericity test: Chi-square = {chi_square:.3f}, p = {p_value:.3f}')

kmo_all, kmo_model = calculate_kmo(data)
print(f'KMO: {kmo_model:.3f}')

# Parallel analysis for number of factors
from factor_analyzer import FactorAnalyzer
fa_parallel = FactorAnalyzer(rotation=None)
fa_parallel.fit(data)
eigenvalues, _ = fa_parallel.get_eigenvalues()

# Plot scree plot
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(eigenvalues)+1), eigenvalues, 'o-')
plt.axhline(y=1, color='r', linestyle='--')
plt.title('Scree Plot')
plt.xlabel('Factor Number')
plt.ylabel('Eigenvalue')
plt.show()

# Factor analysis with rotation
fa = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml')
fa.fit(data)

# Results
loadings = fa.loadings_
communalities = fa.get_communalities()
uniquenesses = fa.get_uniquenesses()
eigenvalues_post = fa.get_eigenvalues()[0]

print("Factor Loadings:")
print(pd.DataFrame(loadings, index=data.columns, 
                   columns=[f'Factor{i+1}' for i in range(3)]))

print("\nCommunalities:")
print(pd.DataFrame(communalities, index=data.columns, columns=['Communality']))
```

\section{R Implementation}

R provides excellent factor analysis capabilities through several packages.

\subsection{psych Package}

The \texttt{psych} package is the most comprehensive R package for factor analysis.

\textbf{Key Functions:}
\begin{itemize}
\item \texttt{fa()}: Main factor analysis function
\item \texttt{fa.parallel()}: Parallel analysis
\item \texttt{VSS()}: Very Simple Structure criterion
\item \texttt{principal()}: Principal component analysis for comparison
\end{itemize}

\textbf{Comprehensive Example:}
```r
library(psych)

# Load data (using built-in dataset)
data(bfi)
bfi_complete <- bfi[complete.cases(bfi[1:25]), 1:25]  # Remove missing data

# Adequacy tests
KMO(bfi_complete)
cortest.bartlett(bfi_complete)

# Parallel analysis
fa.parallel(bfi_complete, fm='ml', fa='fa')

# Factor analysis
fa_result <- fa(bfi_complete, nfactors=5, rotate='varimax', fm='ml')
print(fa_result, cut=0.3, sort=TRUE)

# Oblique rotation
fa_oblique <- fa(bfi_complete, nfactors=5, rotate='oblimin', fm='ml')
print(fa_oblique, cut=0.3, sort=TRUE)

# Factor correlations (for oblique)
fa_oblique$Phi

# Visualization
fa.diagram(fa_result)
```

\subsection{GPArotation Package}

The \texttt{GPArotation} package provides additional rotation methods.

```r
library(GPArotation)

# Available rotation methods
methods <- c("varimax", "quartimax", "equamax", "oblimin", "promax")

# Custom rotation example
fa_custom <- fa(data, nfactors=3, rotate="oblimin", 
                GPArotation.control=list(method="oblimin", gamma=0))
```

\section{SPSS Implementation}

SPSS provides factor analysis through the \texttt{FACTOR} command and GUI menus.

\subsection{GUI Approach}

1. \texttt{Analyze > Dimension Reduction > Factor...}
2. Select variables
3. Choose extraction method (Principal Axis Factoring recommended)
4. Set rotation method (Varimax or Direct Oblimin)
5. Specify options and output

\subsection{Syntax Approach}

```spss
FACTOR
  /VARIABLES var1 var2 var3 var4 var5 var6
  /MISSING LISTWISE 
  /ANALYSIS var1 var2 var3 var4 var5 var6
  /PRINT INITIAL CORRELATION SIG DET KMO EXTRACTION ROTATION
  /FORMAT SORT BLANK(.3)
  /PLOT EIGEN
  /CRITERIA MINEIGEN(1) ITERATE(25)
  /EXTRACTION PAF
  /CRITERIA ITERATE(25) DELTA(0)
  /ROTATION VARIMAX
  /METHOD=CORRELATION.
```

\textbf{Key Options:}
\begin{itemize}
\item \texttt{/EXTRACTION PAF}: Principal Axis Factoring
\item \texttt{/ROTATION VARIMAX}: Varimax rotation
\item \texttt{/ROTATION OBLIMIN}: Direct Oblimin rotation
\item \texttt{/PLOT EIGEN}: Scree plot
\item \texttt{/FORMAT BLANK(.3)}: Suppress loadings < 0.3
\end{itemize}

\section{SAS Implementation}

SAS provides factor analysis through \texttt{PROC FACTOR}.

\subsection{Basic Syntax}

```sas
PROC FACTOR DATA=mydata METHOD=PRIN ROTATE=VARIMAX NFACTORS=3;
  VAR var1-var6;
  RUN;
```

\subsection{Comprehensive Example}

```sas
PROC FACTOR DATA=mydata 
    METHOD=ML          /* Maximum Likelihood */
    ROTATE=PROMAX      /* Promax rotation */
    NFACTORS=3         /* Number of factors */
    SCREE              /* Scree plot */
    RESIDUALS          /* Residual correlations */
    SIMPLE             /* Descriptive statistics */
    CORR;              /* Correlation matrix */
    
  VAR item1-item20;
  
  /* Save factor scores */
  ODS OUTPUT RotatedFactorPattern=loadings;
  ODS OUTPUT FinalCommunalityEstimates=communalities;
RUN;
```

\section{Software Comparison and Validation}

\subsection{Default Differences}

Different software packages have different defaults that can affect results:

\begin{table}[h]
\centering
\caption{Software Default Settings Comparison}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Software} & \textbf{Extraction} & \textbf{Rotation} & \textbf{Communality Init} \\
\midrule
R (psych) & Minimum residual & Varimax & SMC \\
Python (factor\_analyzer) & ML & Varimax & SMC \\
SPSS & Principal Components & Varimax & 1.0 \\
SAS & Principal Components & Varimax & 1.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{learningtip}
Always specify your extraction method, rotation, and other key parameters explicitly rather than relying on defaults. This ensures reproducibility and comparability across software platforms.
\end{learningtip}

\subsection{Validation Strategy}

\begin{enumerate}
\item \textbf{Replicate key analyses} across 2-3 software platforms
\item \textbf{Use identical settings} (extraction method, rotation, convergence criteria)
\item \textbf{Compare major results} (factor loadings, communalities, factor correlations)
\item \textbf{Investigate discrepancies} (different algorithms, numerical precision)
\item \textbf{Report validation} in methodology section
\end{enumerate}

\section{Best Practices for Implementation}

\subsection{Data Preparation}

\textbf{Missing Data:}
\begin{itemize}
\item Listwise deletion: Simple but reduces sample size
\item Pairwise deletion: Uses all available data but can create problems
\item Imputation: Multiple imputation preferred for MAR data
\end{itemize}

\textbf{Outliers:}
\begin{itemize}
\item Examine univariate and multivariate outliers
\item Consider robust estimation methods
\item Document outlier treatment decisions
\end{itemize}

\textbf{Data Adequacy:}
```python
# Python example for data adequacy
from factor_analyzer import calculate_bartlett_sphericity, calculate_kmo

# Bartlett's test of sphericity
chi_square, p_value = calculate_bartlett_sphericity(data)
print(f'Bartlett test: p = {p_value:.6f}')

# Kaiser-Meyer-Olkin test
kmo_all, kmo_model = calculate_kmo(data)
print(f'Overall KMO: {kmo_model:.3f}')

# KMO interpretation
if kmo_model >= 0.9:
    print("Marvelous")
elif kmo_model >= 0.8:
    print("Meritorious")  
elif kmo_model >= 0.7:
    print("Middling")
elif kmo_model >= 0.6:
    print("Mediocre")
else:
    print("Miserable")
```

\subsection{Analysis Workflow}

\begin{algorithm}
\caption{Recommended Factor Analysis Workflow}
\begin{algorithmic}
\STATE 1. \textbf{Data screening:} Missing data, outliers, distributions
\STATE 2. \textbf{Adequacy tests:} KMO, Bartlett's test
\STATE 3. \textbf{Factor extraction:} Multiple methods for comparison
\STATE 4. \textbf{Number of factors:} Kaiser, scree plot, parallel analysis
\STATE 5. \textbf{Rotation:} Try both orthogonal and oblique
\STATE 6. \textbf{Interpretation:} Name factors, examine loadings
\STATE 7. \textbf{Validation:} Cross-validation, external validation
\STATE 8. \textbf{Reporting:} Complete documentation of decisions
\end{algorithmic}
\end{algorithm}

\subsection{Common Implementation Pitfalls}

\begin{commonmistake}
\textbf{Avoid these common mistakes:}
\begin{itemize}
\item \textbf{Using PCA defaults in factor analysis software:} Many programs default to PCA, not true factor analysis
\item \textbf{Not checking data adequacy:} KMO < 0.5 suggests factor analysis is inappropriate
\item \textbf{Over-relying on single software:} Verify important results across platforms
\item \textbf{Ignoring convergence warnings:} Non-convergence indicates problems with the solution
\item \textbf{Not reporting key decisions:} Extraction method, rotation, number of factors criteria
\item \textbf{Using unstable solutions:} Very small samples or poor factor retention decisions
\end{itemize}
\end{commonmistake}

\section{Reporting and Documentation}

\subsection{Essential Reporting Elements}

When reporting factor analysis results, include:

\begin{enumerate}
\item \textbf{Sample characteristics:} Size, demographics, data collection method
\item \textbf{Data adequacy:} KMO value, Bartlett's test results  
\item \textbf{Extraction method:} PAF, ML, etc., with justification
\item \textbf{Factor retention:} Criteria used, number retained, variance explained
\item \textbf{Rotation method:} Type and rationale for choice
\item \textbf{Loading cutoff:} Minimum loading for interpretation (typically 0.32 or 0.40)
\item \textbf{Factor loadings:} Complete matrix or abbreviated with cutoff
\item \textbf{Factor correlations:} For oblique solutions
\item \textbf{Reliability:} Cronbach's alpha for each factor
\item \textbf{Software used:} Version and key settings
\end{enumerate}

\subsection{Tables and Figures}

\textbf{Standard Factor Loading Table:}
```
Table 1
Factor Loadings for [Brief Description] (N = XXX)

                        Factor
Item                    1     2     3    h²
Item 1 description    .xx   .xx   .xx   .xx
Item 2 description    .xx   .xx   .xx   .xx
...

Eigenvalues          x.xx  x.xx  x.xx
% of variance        xx.x  xx.x  xx.x
Cumulative %         xx.x  xx.x  xx.x

Note: Factor loadings < .30 are suppressed. 
Extraction: Principal Axis Factoring. 
Rotation: Varimax.
```

This comprehensive guide provides the foundation for understanding and implementing factor analysis across multiple contexts and software platforms. Remember that factor analysis is as much an art as a science—technical competence must be combined with theoretical understanding and practical judgment to produce meaningful results.

\end{document}