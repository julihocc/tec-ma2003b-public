\documentclass[aspectratio=169]{beamer}

% Presentation metadata
\title{Factor Analysis}
\author{Dr. Juliho Castillo}
\institute{Tecnológico de Monterrey}
\date{\today}

% Additional packages for the presentation
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}

% Use Metropolis theme with a blue color palette
% (Metropolis should be available in the TeX distribution; if not, install the MTheme package.)
\usetheme{metropolis}
% Metro options: tidy title and a subtle progress indicator
\metroset{titleformat=smallcaps,progressbar=frametitle}


\begin{document}

% Title slide
\begin{frame}
    \titlepage
\end{frame}

% Table of contents
\begin{frame}
    \tableofcontents
\end{frame}

% ============================================================================
% PART I: PRINCIPAL COMPONENT ANALYSIS
% ============================================================================

\part{Principal Component Analysis}

\begin{frame}
    \partpage
\end{frame}

\section{Introduction to Multivariate Analysis}

\begin{frame}[fragile]
    \frametitle{Multivariate Analysis Overview}
    \begin{itemize}
        \item \textbf{Multivariate Analysis}: Statistical methods for analyzing multiple variables simultaneously \pause
        \item \textbf{Key Challenge}: Understanding relationships among many correlated variables \pause
        \item \textbf{Two Main Approaches}: 
              \begin{itemize}
                  \item \textit{Principal Component Analysis (PCA)}: Dimensionality reduction technique \pause
                  \item \textit{Factor Analysis}: Latent variable modeling technique \pause
              \end{itemize}
        \item \textbf{This Course}: We'll explore both methods using the same datasets for direct comparison
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Structure Overview}
    \textbf{Part I: Principal Component Analysis}
    \begin{itemize}
        \item PCA theory and mathematical foundation
        \item Four comprehensive examples across different domains
    \end{itemize}
    \vspace{12pt}
    \textbf{Part II: Factor Analysis}
    \begin{itemize}
        \item Factor Analysis theory and modeling approach  
        \item Same datasets analyzed with Factor Analysis
    \end{itemize}
    \vspace{12pt}
    \textbf{Part III: Comparison and Applications}
    \begin{itemize}
        \item Side-by-side comparison of results
        \item Guidelines for choosing the appropriate method
    \end{itemize}
\end{frame}

\section{Principal Component Analysis}

\begin{frame}
    \frametitle{Refresher: What is PCA?}
    \begin{itemize}
        \item Principal Component Analysis (PCA) is a linear method for \textbf{dimension reduction}. \pause
        \item It finds orthogonal directions (principal components) that capture the largest possible variance in the data. \pause
        \item PCA produces new variables (components) that are linear combinations of the original observed variables. \pause
        \item Use cases: visualization, noise reduction, pre-processing before supervised learning, and exploratory data analysis. \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Mathematical formulation}
    Let $\mathbf{x}\in\mathbb{R}^p$ be a random vector with mean $\mu$ and covariance matrix $\Sigma$. After centering the data ($\mathbf{x}-\mu$):
    \begin{itemize}
        \item Find eigenvalues $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p$ and orthonormal eigenvectors $\mathbf{v}_1,\dots,\mathbf{v}_p$ of $\Sigma$: $\Sigma\mathbf{v}_j=\lambda_j\mathbf{v}_j$. \pause
        \item The $j$-th principal component is $z_j=\mathbf{v}_j^{\top}(\mathbf{x}-\mu)$. \pause
        \item Variance explained by component $j$ is $\mathrm{Var}(z_j)=\lambda_j$. The proportion explained is $\lambda_j/\sum_{k=1}^p\lambda_k$. \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Computation steps (practical)}
    \begin{enumerate}
        \item Standardize variables if they are on different scales (use correlation matrix) or center only if scales are comparable (use covariance matrix). \pause
        \item Compute covariance (or correlation) matrix $S$ from the data. \pause
        \item Compute eigen decomposition $S=V\Lambda V^{\top}$. \pause
        \item Form principal component scores: $Z = X_c V$ (where $X_c$ is centered data and columns of $V$ are eigenvectors). \pause
        \item Inspect eigenvalues, cumulative variance, and scree plot to decide how many components to keep. \pause
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Deciding how many components to retain}
    Common heuristics and formal approaches:
    \begin{itemize}
        \item Kaiser criterion: keep components with eigenvalue $>1$ (applies when using correlation matrix). \pause
        \item Cumulative variance: keep the smallest number of components that explain a target (e.g., 70--90\%) of total variance. \pause
        \item Scree plot: look for the "elbow" where additional components contribute little incremental variance. \pause
        \item Parallel analysis: compare empirical eigenvalues to those obtained from random data — keep components with larger eigenvalues than random. \pause
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{PCA vs Factor Analysis (reminder)}
    \begin{itemize}
        \item PCA: descriptive linear combinations that maximize variance; components are exact linear functions of observed variables and need not have a causal or measurement model interpretation. \pause
        \item Factor Analysis: a statistical model that explicitly decomposes observed variance into common (shared) variance explained by latent factors and unique variance (errors). \pause
        \item Practical rule: use PCA for dimension reduction and data compression; use Factor Analysis when your goal is to model latent constructs and separate common from unique variance. \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical tips and pitfalls}
    \begin{itemize}
        \item Always check variable scales; standardize when necessary. \pause
        \item PCA is sensitive to outliers — inspect data and consider robust alternatives if needed. \pause
        \item Interpret components via loadings (eigenvectors) and by examining which variables contribute strongly to each component. \pause
        \item Rotation is not standard in PCA (rotation reassigns variance) — if interpretability is a priority, consider Factor Analysis with rotation. \pause
        \item When reporting, include: eigenvalues table, proportion of variance, cumulative variance, scree plot, and a table of loadings (component matrix). \pause
    \end{itemize}
\end{frame}

\section{Educational Assessment: Synthetic PCA Example}

\begin{frame}
    \frametitle{Educational Assessment: Synthetic PCA Example}
    This section demonstrates PCA using controlled synthetic data with known factor structure to validate the method and teach key concepts.
    \begin{itemize}
        \item \textbf{Dataset}: Student assessment data with 6 variables (100 students) \pause
        \item \textbf{Research Question}: Can PCA recover the underlying ability factors? How does it separate meaningful structure from noise? \pause
        \item \textbf{Method}: Standardized PCA on synthetic data with known latent factors \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{lessons/4\_Factor\_Analysis/code/pca\_example/pca\_example.py}
\end{frame}

\begin{frame}
    \frametitle{Dataset: Student Assessment Variables}
    Six variables representing different aspects of student ability:
    \begin{itemize}
        \item \textbf{MathTest}: Mathematics assessment score \pause
        \item \textbf{VerbalTest}: Verbal reasoning assessment score \pause
        \item \textbf{SocialSkills}: Social competency rating \pause
        \item \textbf{Leadership}: Leadership ability rating \pause
    \end{itemize}
    \vspace{6pt}
    \begin{itemize}
        \item \textbf{RandomVar1}: Pure noise variable (no latent structure) \pause
        \item \textbf{RandomVar2}: Pure noise variable (no latent structure) \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Known Structure:}
    \begin{itemize}
        \item \textit{Intelligence Factor}: Affects MathTest (0.85 loading) and VerbalTest (0.80 loading) \pause
        \item \textit{Personality Factor}: Affects SocialSkills (0.85 loading) and Leadership (0.80 loading) \pause
        \item Measurement error added to all meaningful variables (0.2-0.25 noise levels) \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Context and Validation}
    \textbf{Pedagogical Value:}
    \begin{itemize}
        \item Ground truth known $\rightarrow$ can validate PCA's ability to recover factors \pause
        \item Realistic psychological assessment scenario that students understand \pause
        \item Clear separation between meaningful variables and pure noise \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Expected Learning Outcomes:}
    \begin{itemize}
        \item Understand how PCA handles correlated variables driven by latent factors \pause
        \item See how noise components are separated from meaningful structure \pause
        \item Learn to interpret component loadings in context of known relationships \pause
        \item Practice using scree plots and eigenvalue criteria for component selection \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Factor Recovery}
    When running the analysis, we observe meaningful factor separation:
    \begin{itemize}
        \item \textbf{PC1} (36.7\% variance): General ability factor \pause
              \begin{itemize}
                  \item Eigenvalue $\approx$ 2.2 (well above Kaiser threshold) \pause
                  \item Captures common variance across all meaningful measures \pause
                  \item Reflects "halo effect" common in ability assessments \pause
              \end{itemize}
        \item \textbf{PC2} (30.8\% variance): Specific ability dimensions \pause
              \begin{itemize}
                  \item May separate cognitive from social abilities \pause
                  \item Shows how PCA can capture multiple meaningful factors \pause
              \end{itemize}
        \item \textbf{PC3-PC4} (30.2\% variance): Additional structure and measurement error \pause
        \item \textbf{PC5-PC6} (2.3\% variance): Pure noise components \pause
              \begin{itemize}
                  \item Very low eigenvalues ($<$ 0.15) clearly identify noise floor \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Interpretation: Ability Structure}
    \textbf{PC1 as General Ability:}
    \begin{itemize}
        \item Strong loadings on all meaningful variables (MathTest, VerbalTest, SocialSkills, Leadership) \pause
        \item Weak loadings on noise variables (RandomVar1, RandomVar2) \pause
        \item Demonstrates how PCA identifies common underlying factors \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Validation Success:}
    \begin{itemize}
        \item \textit{Factor Recovery}: Meaningful variables show loading strength $\sim$0.45-0.50 \pause
        \item \textit{Noise Separation}: Random variables show much weaker loadings $<$0.33 \pause
        \item \textit{Structure Detection}: Clear eigenvalue drop separates signal from noise \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scree Plot: Learning Component Selection}
    The scree plot demonstrates key decision-making concepts:
    \begin{itemize}
        \item Eigenvalues decline from meaningful structure ($>$0.8) to pure noise ($<$0.15) \pause
        \item Kaiser criterion suggests retaining 4 components ($\lambda > 1$) \pause
        \item 67\% variance explained by first 2 components is realistic for assessment data \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Teaching Points:}
    \begin{itemize}
        \item No single "correct" number of components — depends on purpose \pause
        \item Dramatic eigenvalue drop after PC4 indicates transition to noise \pause
        \item Real data rarely shows as clean separation as this synthetic example \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Biplot: Students and Abilities}
    The biplot reveals educational assessment patterns:
    \begin{itemize}
        \item \textbf{Variable arrows}: Show how different abilities relate to PC dimensions \pause
              \begin{itemize}
                  \item Academic tests (Math, Verbal) cluster together if similar factor loadings \pause
                  \item Social measures (Skills, Leadership) may point in different direction \pause
                  \item Random variables show smaller, more scattered loadings \pause
              \end{itemize}
        \item \textbf{Student points}: Individual students positioned in ability space \pause
              \begin{itemize}
                  \item Spread along PC1 shows general ability differences \pause
                  \item Students with high PC1 scores excel across multiple domains \pause
                  \item Can identify students with specific ability profiles \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Applications}
    \textbf{Assessment Development:}
    \begin{itemize}
        \item Validate that tests measure intended constructs (factor loadings) \pause
        \item Identify redundant measures that capture similar abilities \pause
        \item Design balanced assessments across multiple ability dimensions \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Student Analysis:}
    \begin{itemize}
        \item Create composite ability scores using PC1 weights \pause
        \item Identify students with unusual ability profiles (outliers in PC space) \pause
        \item Track ability development over time using consistent PC scoring \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Research Applications:}
    \begin{itemize}
        \item Test theories about ability structure and factor relationships \pause
        \item Compare ability patterns across different populations or contexts \pause
        \item Develop more efficient assessment batteries based on factor structure \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Code Example: Synthetic Data Analysis}
    \begin{itemize}
        \item \textbf{Data generation}: Creates realistic educational assessment scenario with known factors \pause
        \item \textbf{Main analysis}: \texttt{pca\_example.py} performs PCA with detailed pedagogical interpretation \pause
        \item \textbf{Outputs}:
              \begin{itemize}
                  \item Eigenvalues, explained variance ratios, factor recovery validation \pause
                  \item \texttt{pca\_scree.png}: Scree plot showing signal-to-noise transition \pause
                  \item \texttt{pca\_biplot.png}: Students and abilities in PC space \pause
                  \item Student rankings by PC scores for practical interpretation \pause
              \end{itemize}
        \item \textbf{Usage}: \texttt{cd code/pca\_example \&\& python pca\_example.py} \pause
    \end{itemize}
    \vspace{6pt}
    This example is ideal for learning PCA concepts because the ground truth is known, making it easy to validate whether the method works as expected.
\end{frame}

\section{Investment allocation example}

\begin{frame}
    \frametitle{European Stock Markets: PCA Analysis}
    This section demonstrates PCA applied to financial markets using synthetic European stock market data.
    \begin{itemize}
        \item \textbf{Dataset}: 4 major European indices (DAX, SMI, CAC, FTSE) over 1,860 trading days. \pause
        \item \textbf{Research Question}: How integrated are European financial markets? Can we identify common market factors? \pause
        \item \textbf{Method}: Standardized PCA on correlation matrix of daily returns. \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{lessons/4\_Factor\_Analysis/code/invest\_example/invest\_example.py}
\end{frame}

\begin{frame}
    \frametitle{Dataset: European Market Indices}
    \begin{itemize}
        \item \textbf{DAX (Germany)}: Frankfurt Stock Exchange — largest European economy \pause
        \item \textbf{SMI (Switzerland)}: Swiss Market Index — major financial center \pause
        \item \textbf{CAC (France)}: Paris Stock Exchange — core eurozone market \pause
        \item \textbf{FTSE (UK)}: London Stock Exchange — major international hub \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Financial Context:}
    \begin{itemize}
        \item Daily returns standardized (mean=0, std=1) for scale-invariant analysis \pause
        \item Captures correlation structure across different currencies and economies \pause
        \item Reflects market integration through EU membership and globalization \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Market Integration}
    When running the analysis, we observe:
    \begin{itemize}
        \item \textbf{PC1}: Explains $\sim$97\% of total variance \pause
              \begin{itemize}
                  \item Eigenvalue $\approx$ 3.9 (well above Kaiser threshold of 1.0) \pause
                  \item Represents a \textit{common European market factor} \pause
                  \item All markets load positively — they move together \pause
              \end{itemize}
        \item \textbf{PC2-PC4}: Explain only $\sim$3\% combined variance \pause
              \begin{itemize}
                  \item Capture market-specific idiosyncrasies \pause
                  \item Currency effects, country-specific political events \pause
                  \item Largely noise for portfolio construction \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Financial Interpretation: Market Factor}
    \textbf{PC1 as a Market Factor:}
    \begin{itemize}
        \item Captures \textit{systematic risk} — movements common to all markets \pause
        \item Driven by: EU-wide economic conditions, global financial sentiment, major central bank policies \pause
        \item High loadings on all indices $\rightarrow$ European markets are highly integrated \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Implications for Portfolio Management:}
    \begin{itemize}
        \item \textit{Limited diversification benefit} from spreading across European markets alone \pause
        \item Most portfolio variance comes from exposure to the common factor \pause
        \item Geographic diversification requires markets with different factor exposures \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Visualization: Scree Plot Analysis}
    The scree plot shows a dramatic \textit{``cliff''} pattern:
    \begin{itemize}
        \item Sharp drop from PC1 ($\lambda \approx 3.9$) to PC2 ($\lambda \approx 0.09$) \pause
        \item Clear ``elbow'' indicates one dominant factor \pause
        \item Remaining components are essentially flat (noise floor) \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Decision Rule:}
    \begin{itemize}
        \item Kaiser criterion: Retain PC1 only ($\lambda > 1$) \pause
        \item Variance threshold: PC1 alone exceeds any reasonable cutoff (80\%, 90\%, 95\%) \pause
        \item Practical conclusion: European markets can be summarized by a single factor \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Biplot: Markets and Time Periods}
    The biplot reveals the factor structure:
    \begin{itemize}
        \item \textbf{Variable arrows} (red): Show market loadings on PC1-PC2 \pause
              \begin{itemize}
                  \item All arrows point in similar direction $\rightarrow$ positive correlation \pause
                  \item Arrow length reflects contribution to variance \pause
                  \item Angle between arrows shows correlation strength \pause
              \end{itemize}
        \item \textbf{Observation points}: Individual trading days in PC space \pause
              \begin{itemize}
                  \item Horizontal spread (PC1): Common market movements \pause
                  \item Vertical spread (PC2): Minor market-specific deviations \pause
                  \item Outliers may represent crisis periods or major events \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical Applications}
    \textbf{Risk Management:}
    \begin{itemize}
        \item Use PC1 scores as a single \textit{European market risk factor} \pause
        \item Portfolio $\beta$ to PC1 determines systematic risk exposure \pause
        \item Stress testing: model extreme PC1 movements \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Portfolio Construction:}
    \begin{itemize}
        \item Market-neutral strategies require offsetting PC1 exposure \pause
        \item Alpha generation focuses on PC2-PC4 (idiosyncratic components) \pause
        \item Diversification requires assets uncorrelated with European factor \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Performance Attribution:}
    \begin{itemize}
        \item Decompose returns into market factor (PC1) + specific factors (PC2+) \pause
        \item Distinguish skill from market timing \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Code Example: Running the Analysis}
    \begin{itemize}
        \item \textbf{Data preparation}: \texttt{fetch\_invest.py} generates synthetic European market data \pause
        \item \textbf{Main analysis}: \texttt{invest\_example.py} performs PCA with detailed financial interpretation \pause
        \item \textbf{Outputs}: 
              \begin{itemize}
                  \item Eigenvalues, explained variance ratios, cumulative variance \pause
                  \item \texttt{invest\_scree.png}: Scree plot for component selection \pause
                  \item \texttt{invest\_biplot.png}: Biplot of markets and time periods \pause
              \end{itemize}
        \item \textbf{Usage}: \texttt{cd code/invest\_example \&\& python invest\_example.py} \pause
    \end{itemize}
    \vspace{6pt}
    The script includes detailed py-percent comments for interactive exploration and financial interpretation of all results.
\end{frame}

\section{Kuiper Belt Objects: Astronomical PCA}

\begin{frame}
    \frametitle{Kuiper Belt Objects: PCA Analysis}
    This section demonstrates PCA applied to astronomical data from the outer solar system.
    \begin{itemize}
        \item \textbf{Dataset}: Orbital parameters of 98 trans-Neptunian objects (TNOs) and Kuiper Belt objects \pause
        \item \textbf{Research Question}: What are the main modes of orbital variation? Can we identify distinct dynamical populations? \pause
        \item \textbf{Method}: Standardized PCA on 5 orbital elements with different physical units \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{lessons/4\_Factor\_Analysis/code/kuiper\_example/kuiper\_example.py}
\end{frame}

\begin{frame}
    \frametitle{Dataset: Orbital Parameters}
    Five key orbital elements describe each object's motion:
    \begin{itemize}
        \item \textbf{a} (AU): Semi-major axis — average distance from Sun (30-150 AU) \pause
        \item \textbf{e}: Eccentricity — orbital shape (0=circle, 1=parabola) \pause
        \item \textbf{i} (degrees): Inclination — tilt relative to solar system plane \pause
        \item \textbf{H} (magnitude): Absolute magnitude — brightness/size indicator \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Physical Context:}
    \begin{itemize}
        \item Objects beyond Neptune's orbit ($\sim$30 AU) in the outer solar system \pause
        \item Different units require standardization for meaningful PCA \pause
        \item Orbital correlations reflect gravitational interactions and formation history \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Dynamical Populations in the Kuiper Belt}
    Three main populations with distinct orbital signatures:
    \begin{itemize}
        \item \textbf{Classical Kuiper Belt} (60\%): Low eccentricity, low inclination \pause
              \begin{itemize}
                  \item Nearly circular orbits around 39-48 AU \pause
                  \item ``Cold'' population — likely formed in place \pause
              \end{itemize}
        \item \textbf{Scattered Disk Objects} (30\%): High eccentricity, distant \pause
              \begin{itemize}
                  \item $e > 0.3$, semi-major axis $> 50$ AU \pause
                  \item Scattered outward by gravitational encounters with Neptune \pause
              \end{itemize}
        \item \textbf{Resonant Objects} (10\%): Locked in orbital resonances \pause
              \begin{itemize}
                  \item 3:2 resonance at $\sim$39.4 AU (like Pluto) \pause
                  \item Captured during Neptune's outward migration \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Orbital Excitation}
    When running the analysis, we observe a more balanced variance distribution:
    \begin{itemize}
        \item \textbf{PC1} (36.5\% variance): Orbital excitation dimension \pause
              \begin{itemize}
                  \item Correlates semi-major axis, eccentricity, and inclination \pause
                  \item Separates dynamically ``hot'' (excited) from ``cold'' (pristine) populations \pause
              \end{itemize}
        \item \textbf{PC2} (23.2\% variance): Size-distance relationship \pause
              \begin{itemize}
                  \item May reflect observational bias or physical size distribution \pause
              \end{itemize}
        \item \textbf{PC3} (17.8\% variance): Additional orbital structure \pause
              \begin{itemize}
                  \item First 3 components explain $\sim$77.5\% of variance \pause
                  \item More complex structure than financial markets example \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Astronomical Interpretation: Dynamical Evolution}
    \textbf{PC1 as Dynamical Excitation:}
    \begin{itemize}
        \item High loadings on distance (a), eccentricity (e), and inclination (i) \pause
        \item Represents gravitational ``heating'' of orbits over solar system history \pause
        \item Separates pristine objects from those scattered by planetary migration \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Astrophysical Implications:}
    \begin{itemize}
        \item \textit{Formation models}: PC1 scores distinguish formation mechanisms \pause
        \item \textit{Dynamical families}: Objects with similar PC scores likely share evolutionary history \pause
        \item \textit{Size segregation}: Large objects may preferentially survive in certain orbital regions \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scree Plot: Multiple Components Matter}
    Unlike financial markets, the Kuiper Belt shows more distributed variance:
    \begin{itemize}
        \item Gradual decline rather than sharp cliff — no single dominant factor \pause
        \item First eigenvalue $\sim$1.8, others $\sim$1.2, 0.9, 0.8 (above noise level) \pause
        \item Suggests 2-3 meaningful components for dimensional reduction \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Decision Rules:}
    \begin{itemize}
        \item Kaiser criterion: Retain 2 components ($\lambda > 1$) \pause
        \item 80\% variance threshold: Need 3 components (77.5\% with 3) \pause
        \item Physical interpretation: Multiple gravitational processes create complex structure \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Biplot: Objects in Orbital Space}
    The biplot reveals orbital relationships:
    \begin{itemize}
        \item \textbf{Variable arrows}: Orbital parameter loadings \pause
              \begin{itemize}
                  \item Clustered arrows (a, e, i) show correlated excitation \pause
                  \item H (brightness) may point differently — size-orbital coupling \pause
              \end{itemize}
        \item \textbf{Object points}: Individual Kuiper Belt objects in PC space \pause
              \begin{itemize}
                  \item Clustering reveals dynamical families \pause
                  \item Outliers represent unusual objects (highly eccentric, large inclination) \pause
                  \item Can identify candidates for detailed follow-up observations \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scientific Applications}
    \textbf{Population Studies:}
    \begin{itemize}
        \item Classify objects into dynamical families using PC scores \pause
        \item Test formation and migration models against observed distributions \pause
        \item Identify rare populations (detached objects, extreme resonances) \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Observational Planning:}
    \begin{itemize}
        \item Target unusual objects (outliers in PC space) for detailed study \pause
        \item Optimize survey strategies based on population structure \pause
        \item Predict undiscovered populations in unsampled orbital regions \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Comparative Planetology:}
    \begin{itemize}
        \item Compare our solar system structure to exoplanetary debris disks \pause
        \item Understand how planetary migration shapes outer system architecture \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Code Example: Astronomical Data Analysis}
    \begin{itemize}
        \item \textbf{Data generation}: \texttt{fetch\_kuiper.py} creates synthetic orbital database \pause
        \item \textbf{Main analysis}: \texttt{kuiper\_example.py} performs PCA with astronomical interpretation \pause
        \item \textbf{Outputs}:
              \begin{itemize}
                  \item Eigenvalues showing distributed variance structure \pause
                  \item \texttt{kuiper\_scree.png}: Scree plot for component selection \pause
                  \item \texttt{kuiper\_biplot.png}: Objects and orbital parameters in PC space \pause
              \end{itemize}
        \item \textbf{Usage}: \texttt{cd code/kuiper\_example \&\& python kuiper\_example.py} \pause
    \end{itemize}
    \vspace{6pt}
    The script demonstrates PCA applied to scientific data with multiple meaningful components and physical interpretation of mathematical results.
\end{frame}

\section{Hospital Health Outcomes}

\begin{frame}
    \frametitle{Hospital Health Outcomes: PCA Analysis}
    This section demonstrates PCA applied to healthcare quality assessment using hospital performance metrics.
    \begin{itemize}
        \item \textbf{Dataset}: 8 key performance indicators from 50 US hospitals \pause
        \item \textbf{Research Question}: What are the main dimensions of hospital quality? Can we identify overall performance patterns? \pause
        \item \textbf{Method}: Standardized PCA on health outcome metrics with different units \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{lessons/4\_Factor\_Analysis/code/hospitals\_example/hospitals\_example.py}
\end{frame}

\begin{frame}
    \frametitle{Dataset: Hospital Performance Metrics}
    Eight key hospital quality indicators across multiple domains:
    \begin{itemize}
        \item \textbf{MortalityRate} (\%): Hospital mortality rate — lower is better \pause
        \item \textbf{ReadmissionRate} (\%): 30-day readmission rate — lower is better \pause
        \item \textbf{PatientSatisfaction} (0-100): HCAHPS survey score — higher is better \pause
        \item \textbf{AvgLengthStay} (days): Average inpatient stay — shorter generally better \pause
    \end{itemize}
    \vspace{6pt}
    \begin{itemize}
        \item \textbf{InfectionRate} (\%): Hospital-acquired infections — lower is better \pause
        \item \textbf{NurseRatio}: Nurse-to-patient ratio — higher is better \pause
        \item \textbf{SurgicalComplications} (\%): Surgical complication rate — lower is better \pause
        \item \textbf{EDWaitTime} (minutes): Emergency department wait time — lower is better \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Healthcare Quality Context}
    \textbf{Clinical Significance:}
    \begin{itemize}
        \item Metrics reflect different aspects of hospital performance \pause
        \item Used by CMS Hospital Compare and Value-Based Purchasing programs \pause
        \item Different units require standardization for meaningful PCA \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Quality Improvement Context:}
    \begin{itemize}
        \item Hospitals with good management typically excel across multiple metrics \pause
        \item ``Halo effect'': organizational excellence affects all care aspects \pause
        \item Understanding correlation patterns guides quality improvement strategies \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Quality Factor}
    When running the analysis, we observe a dominant first component:
    \begin{itemize}
        \item \textbf{PC1} (70.5\% variance): Overall hospital quality factor \pause
              \begin{itemize}
                  \item Eigenvalue $\approx$ 5.6 (well above Kaiser threshold) \pause
                  \item High loadings on all quality metrics in expected direction \pause
                  \item Represents the ``halo effect'' of organizational excellence \pause
              \end{itemize}
        \item \textbf{PC2} (8.5\% variance): Efficiency vs. thoroughness trade-offs \pause
              \begin{itemize}
                  \item May separate different care philosophies or patient populations \pause
                  \item Captures length-of-stay patterns and resource utilization \pause
              \end{itemize}
        \item \textbf{PC3-PC8}: Explain remaining 21\% variance \pause
              \begin{itemize}
                  \item Capture specific aspects of care not explained by general quality \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Healthcare Interpretation: Quality Assessment}
    \textbf{PC1 as Overall Quality:}
    \begin{itemize}
        \item High negative loadings on ``bad'' metrics (mortality, readmissions, infections, complications, wait times) \pause
        \item High positive loadings on ``good'' metrics (satisfaction, nurse ratios) \pause
        \item Reflects systematic differences in organizational effectiveness \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Implications for Healthcare Management:}
    \begin{itemize}
        \item \textit{Quality is largely unidimensional}: Good hospitals excel across multiple metrics \pause
        \item \textit{Composite scoring}: Use PC1 scores to rank hospital performance \pause
        \item \textit{Targeted improvement}: Focus on organizational excellence rather than isolated metrics \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Visualization: Quality Performance Structure}
    The scree plot shows a clear quality hierarchy:
    \begin{itemize}
        \item Dramatic drop from PC1 ($\lambda \approx 5.6$) to PC2 ($\lambda \approx 0.7$) \pause
        \item Clear ``elbow'' indicates one dominant quality factor \pause
        \item Remaining components are much smaller (noise or specific aspects) \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Decision Rules:}
    \begin{itemize}
        \item Kaiser criterion: Retain PC1 only ($\lambda > 1$) \pause
        \item 70\% variance: PC1 alone exceeds most reasonable cutoffs \pause
        \item Practical conclusion: Hospital quality can be summarized by a single composite score \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Biplot: Hospitals and Quality Metrics}
    The biplot reveals the quality structure:
    \begin{itemize}
        \item \textbf{Variable arrows} (red): Show metric loadings on PC1-PC2 \pause
              \begin{itemize}
                  \item ``Bad'' metrics point left (negative PC1 direction) \pause
                  \item ``Good'' metrics point right (positive PC1 direction) \pause
                  \item Clear separation confirms quality interpretation \pause
              \end{itemize}
        \item \textbf{Hospital points}: Individual hospitals in quality space \pause
              \begin{itemize}
                  \item Horizontal spread (PC1): Overall quality differences \pause
                  \item Right side: High-quality hospitals \pause
                  \item Left side: Hospitals needing quality improvement \pause
                  \item Outliers may represent specialized care models \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Healthcare Applications}
    \textbf{Quality Assessment:}
    \begin{itemize}
        \item Use PC1 scores as composite quality ratings \pause
        \item Identify high-performing hospitals for best practice sharing \pause
        \item Target low-performing hospitals for quality improvement initiatives \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Healthcare Management:}
    \begin{itemize}
        \item Benchmark hospital performance against peer institutions \pause
        \item Guide resource allocation toward high-impact quality initiatives \pause
        \item Track longitudinal quality improvement over time \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Policy and Research:}
    \begin{itemize}
        \item Validate against CMS Hospital Star Ratings and other external measures \pause
        \item Study relationships between quality and operational characteristics \pause
        \item Support value-based purchasing and accountable care initiatives \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Code Example: Healthcare Quality Analysis}
    \begin{itemize}
        \item \textbf{Data preparation}: \texttt{fetch\_hospitals.py} generates synthetic hospital quality data \pause
        \item \textbf{Main analysis}: \texttt{hospitals\_example.py} performs PCA with healthcare interpretation \pause
        \item \textbf{Outputs}:
              \begin{itemize}
                  \item Eigenvalues, explained variance, hospital quality rankings \pause
                  \item \texttt{hospitals\_scree.png}: Scree plot for component selection \pause
                  \item \texttt{hospitals\_biplot.png}: Hospitals and quality metrics in PC space \pause
              \end{itemize}
        \item \textbf{Usage}: \texttt{cd code/hospitals\_example \&\& python hospitals\_example.py} \pause
    \end{itemize}
    \vspace{6pt}
    The script demonstrates PCA applied to healthcare quality assessment with practical interpretation for hospital management and policy applications.
\end{frame}

% ============================================================================
% PART II: FACTOR ANALYSIS
% ============================================================================

\part{Factor Analysis}

\begin{frame}
    \partpage
\end{frame}

\section{Introduction to Factor Analysis}

\begin{frame}[fragile]
    \frametitle{What is Factor Analysis?}
    \begin{itemize}
        \item A statistical method for modeling relationships among \textbf{observed variables} using \textbf{latent factors}. \pause
        \item It uses a smaller number of \textit{unobserved variables}, known as \textbf{common factors}. \pause
        \item \textbf{Key Distinction from PCA}: Explicitly models measurement error and unique variance \pause
        \item Often used to discover and validate underlying theoretical constructs
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Factor Analysis Model}
    \begin{itemize}
        \item \textbf{Common Factors}: Latent variables that influence multiple observed variables \pause
        \item \textbf{Factor Loadings}: Relationships between observed variables and common factors \pause
        \item \textbf{Unique Factors}: Variable-specific variance not explained by common factors \pause
        \item \textbf{Core Assumption}: $X_i = \lambda_{i1}F_1 + \lambda_{i2}F_2 + \cdots + \lambda_{ik}F_k + U_i$
              \begin{itemize}
                  \item $X_i$ = observed variable, $F_j$ = common factors, $U_i$ = unique factor
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Factor Analysis vs. PCA: Key Differences}
    \begin{center}
    \begin{tabular}{|p{5cm}|p{5cm}|}
    \hline
    \textbf{Principal Component Analysis} & \textbf{Factor Analysis} \\
    \hline
    Dimensionality reduction & Latent variable modeling \\
    \hline
    Components are linear combinations of all variables & Factors are hypothetical constructs \\
    \hline
    Explains total variance & Explains common variance only \\
    \hline
    No measurement error model & Explicitly models unique variance \\
    \hline
    Descriptive technique & Statistical model with assumptions \\
    \hline
    \end{tabular}
    \end{center}
    \vspace{12pt}
    \textbf{Next}: We'll analyze the same four datasets with Factor Analysis to see these differences in practice!
\end{frame}

\section{Factor Analysis: Educational Assessment Example}

\begin{frame}
    \frametitle{Educational Assessment: Factor Analysis}
    \textbf{Reanalyzing our synthetic student data with Factor Analysis}
    \begin{itemize}
        \item \textbf{Same Dataset}: 100 students, 6 variables (MathTest, VerbalTest, SocialSkills, Leadership, RandomVar1, RandomVar2) \pause
        \item \textbf{Known Structure}: Intelligence factor + Personality factor + noise \pause
        \item \textbf{FA Advantage}: Should better identify the true 2-factor structure \pause
        \item \textbf{Comparison Goal}: See how FA handles measurement error vs PCA
    \end{itemize}
    \vspace{12pt}
    \textbf{Expected FA Results}:
    \begin{itemize}
        \item Factor 1: Intelligence (Math, Verbal tests) \pause
        \item Factor 2: Personality (Social skills, Leadership) \pause
        \item Random variables should show low communalities \pause
    \end{itemize}
\end{frame}

% Placeholder for additional FA examples and theory
% Will be expanded with Factor Analysis implementations

% ============================================================================
% PART III: COMPARISON AND APPLICATIONS  
% ============================================================================

\part{Comparison and Applications}

\begin{frame}
    \partpage
\end{frame}

\section{PCA vs Factor Analysis: Direct Comparison}

\begin{frame}
    \frametitle{Method Comparison Overview}
    \textbf{We will compare PCA and Factor Analysis results for:}
    \begin{itemize}
        \item \textbf{Educational Assessment}: Known 2-factor structure with noise
        \item \textbf{European Stock Markets}: High correlation, potential single market factor  
        \item \textbf{Kuiper Belt Objects}: Natural population structure in astronomy
        \item \textbf{Hospital Quality}: Healthcare performance measurement
    \end{itemize}
    \vspace{12pt}
    \textbf{Comparison Criteria}:
    \begin{itemize}
        \item Variance explained vs factor interpretability
        \item Treatment of measurement error and unique variance
        \item Practical applications and domain fit
    \end{itemize}
\end{frame}

\section{Guidelines for Method Selection}

\begin{frame}
    \frametitle{When to Use PCA vs Factor Analysis}
    \textbf{Use PCA when:}
    \begin{itemize}
        \item Primary goal is dimensionality reduction \pause
        \item You want to maximize variance explained \pause
        \item Data compression or noise reduction is the objective \pause
        \item You don't have strong theoretical expectations about latent constructs \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Use Factor Analysis when:}
    \begin{itemize}
        \item You want to model latent constructs or theoretical factors \pause
        \item Understanding measurement error and unique variance is important \pause
        \item You need factor rotation for cleaner interpretation \pause
        \item Confirmatory analysis of hypothesized factor structures \pause
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Recommendations}
    \begin{itemize}
        \item \textbf{Start with EDA}: Use PCA first to understand your data structure \pause
        \item \textbf{Theory-driven analysis}: Apply Factor Analysis when you have theoretical expectations \pause
        \item \textbf{Compare both methods}: Results convergence strengthens conclusions \pause
        \item \textbf{Consider sample size}: Factor Analysis requires larger samples \pause
        \item \textbf{Validate results}: Use cross-validation, external criteria, or confirmatory approaches
    \end{itemize}
    \vspace{12pt}
    \textbf{A Word of Caution}:
    \begin{itemize}
        \item Neither method can \textit{prove} a factor structure is correct \pause
        \item Multiple equally valid models may exist for the same dataset \pause
        \item Always combine statistical results with domain knowledge
    \end{itemize}
\end{frame}

\end{document}
