\documentclass[aspectratio=169]{beamer}

% Presentation metadata
\title{Factor Analysis}
\author{Dr. Juliho Castillo}
\institute{Tecnológico de Monterrey}
\date{\today}

% Additional packages for the presentation
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}

% Use Metropolis theme with a blue color palette
% (Metropolis should be available in the TeX distribution; if not, install the MTheme package.)
\usetheme{metropolis}
% Metro options: tidy title and a subtle progress indicator
\metroset{titleformat=smallcaps,progressbar=frametitle}


\begin{document}

% Title slide
\begin{frame}
    \titlepage
\end{frame}

% Table of contents
\begin{frame}
    \tableofcontents
\end{frame}

% ============================================================================
% PART I: THEORETICAL FOUNDATIONS
% ============================================================================

\part{Theoretical Foundations}

\begin{frame}
    \partpage
\end{frame}

\section{Introduction to Multivariate Analysis}

\begin{frame}[fragile]
    \frametitle{Multivariate Analysis Overview}
    \begin{itemize}
        \item \textbf{Multivariate Analysis}: Statistical methods for analyzing multiple variables simultaneously \pause
        \item \textbf{Key Challenge}: Understanding relationships among many correlated variables \pause
        \item \textbf{Two Main Approaches}: 
              \begin{itemize}
                  \item \textit{Principal Component Analysis (PCA)}: Dimensionality reduction technique \pause
                  \item \textit{Factor Analysis}: Latent variable modeling technique \pause
              \end{itemize}
        \item \textbf{This Course}: We'll explore both methods using the same datasets for direct comparison
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Structure Overview}
    \textbf{New Interleaved Structure:}
    \begin{itemize}
        \item Part I: Theoretical foundations (PCA theory)
        \item Part II: Example 1 - Educational Assessment (PCA → FA → Comparison)
        \item Part III: Example 2 - European Stock Markets (PCA → FA → Comparison)
        \item Part IV: Example 3 - Kuiper Belt Objects (PCA → FA → Comparison)
        \item Part V: Example 4 - Hospital Health Outcomes (PCA → FA → Comparison)
        \item Part VI: Overall guidelines and conclusions
    \end{itemize}
\end{frame}

\section{Principal Component Analysis Theory}

\begin{frame}
    \frametitle{Refresher: What is PCA?}
    \begin{itemize}
        \item Principal Component Analysis (PCA) is a linear method for \textbf{dimension reduction}. \pause
        \item It finds orthogonal directions (principal components) that capture the largest possible variance in the data. \pause
        \item PCA produces new variables (components) that are linear combinations of the original observed variables. \pause
        \item Use cases: visualization, noise reduction, pre-processing before supervised learning, and exploratory data analysis. \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Mathematical formulation}
    Let $\mathbf{x}\in\mathbb{R}^p$ be a random vector with mean $\mu$ and covariance matrix $\Sigma$. After centering the data ($\mathbf{x}-\mu$):
    \begin{itemize}
        \item Find eigenvalues $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p$ and orthonormal eigenvectors $\mathbf{v}_1,\dots,\mathbf{v}_p$ of $\Sigma$: $\Sigma\mathbf{v}_j=\lambda_j\mathbf{v}_j$. \pause
        \item The $j$-th principal component is $z_j=\mathbf{v}_j^{\top}(\mathbf{x}-\mu)$. \pause
        \item Variance explained by component $j$ is $\mathrm{Var}(z_j)=\lambda_j$. The proportion explained is $\lambda_j/\sum_{k=1}^p\lambda_k$. \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Computation steps (practical)}
    \begin{enumerate}
        \item Standardize variables if they are on different scales (use correlation matrix) or center only if scales are comparable (use covariance matrix). \pause
        \item Compute covariance (or correlation) matrix $S$ from the data. \pause
        \item Compute eigen decomposition $S=V\Lambda V^{\top}$. \pause
        \item Form principal component scores: $Z = X_c V$ (where $X_c$ is centered data and columns of $V$ are eigenvectors). \pause
        \item Inspect eigenvalues, cumulative variance, and scree plot to decide how many components to keep. \pause
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Deciding how many components to retain}
    Common heuristics and formal approaches:
    \begin{itemize}
        \item Kaiser criterion: keep components with eigenvalue $>1$ (applies when using correlation matrix). \pause
        \item Cumulative variance: keep the smallest number of components that explain a target (e.g., 70--90\%) of total variance. \pause
        \item Scree plot: look for the "elbow" where additional components contribute little incremental variance. \pause
        \item Parallel analysis: compare empirical eigenvalues to those obtained from random data — keep components with larger eigenvalues than random. \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical tips and pitfalls}
    \begin{itemize}
        \item Always check variable scales; standardize when necessary. \pause
        \item PCA is sensitive to outliers — inspect data and consider robust alternatives if needed. \pause
        \item Interpret components via loadings (eigenvectors) and by examining which variables contribute strongly to each component. \pause
        \item Rotation is not standard in PCA (rotation reassigns variance) — if interpretability is a priority, consider Factor Analysis with rotation. \pause
        \item When reporting, include: eigenvalues table, proportion of variance, cumulative variance, scree plot, and a table of loadings (component matrix). \pause
    \end{itemize}
\end{frame}

% ============================================================================
% PART II: EXAMPLE 1 - EDUCATIONAL ASSESSMENT
% ============================================================================

\part{Example 1: Educational Assessment}

\begin{frame}
    \partpage
\end{frame}

\section{Educational Assessment: PCA Analysis}

\begin{frame}
    \frametitle{Educational Assessment: Synthetic PCA Example}
    This section demonstrates PCA using controlled synthetic data with known factor structure to validate the method and teach key concepts.
    \begin{itemize}
        \item \textbf{Dataset}: Student assessment data with 6 variables (100 students) \pause
        \item \textbf{Research Question}: Can PCA recover the underlying ability factors? How does it separate meaningful structure from noise? \pause
        \item \textbf{Method}: Standardized PCA on synthetic data with known latent factors \pause
    \end{itemize}
    \vspace{6pt}
    Scripts: \texttt{educational\_pca.py} (PCA) | \texttt{educational\_fa.py} (FA comparison)
\end{frame}

\begin{frame}
    \frametitle{Dataset: Student Assessment Variables}
    Six variables representing different aspects of student ability:
    \begin{itemize}
        \item \textbf{MathTest}: Mathematics assessment score \pause
        \item \textbf{VerbalTest}: Verbal reasoning assessment score \pause
        \item \textbf{SocialSkills}: Social competency rating \pause
        \item \textbf{Leadership}: Leadership ability rating \pause
        \item \textbf{RandomVar1, RandomVar2}: Pure noise controls \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Known Factor Structure}
    \textbf{Ground Truth for Validation:}
    \begin{itemize}
        \item \textit{Intelligence Factor}: Affects MathTest (0.85 loading) and VerbalTest (0.80 loading) \pause
        \item \textit{Personality Factor}: Affects SocialSkills (0.85 loading) and Leadership (0.80 loading) \pause
        \item Measurement error added to all meaningful variables (0.2-0.25 noise levels) \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Factor Recovery}
    When running the analysis, we observe meaningful factor separation:
    \begin{itemize}
        \item \textbf{PC1} (36.7\% variance): General ability factor \pause
              \begin{itemize}
                  \item Eigenvalue $\approx$ 2.2 (well above Kaiser threshold) \pause
                  \item Captures common variance across all meaningful measures \pause
                  \item Reflects "halo effect" common in ability assessments \pause
              \end{itemize}
        \item \textbf{PC2} (30.8\% variance): Specific ability dimensions \pause
              \begin{itemize}
                  \item May separate cognitive from social abilities \pause
                  \item Shows how PCA can capture multiple meaningful factors \pause
              \end{itemize}
        \item \textbf{PC3-PC4} (30.2\% variance): Additional structure and measurement error \pause
        \item \textbf{PC5-PC6} (2.3\% variance): Pure noise components \pause
              \begin{itemize}
                  \item Very low eigenvalues ($<$ 0.15) clearly identify noise floor \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\section{Factor Analysis Theory}

\begin{frame}[fragile]
    \frametitle{What is Factor Analysis?}
    \begin{itemize}
        \item A statistical method for modeling relationships among \textbf{observed variables} using \textbf{latent factors}. \pause
        \item It uses a smaller number of \textit{unobserved variables}, known as \textbf{common factors}. \pause
        \item \textbf{Key Distinction from PCA}: Explicitly models measurement error and unique variance \pause
        \item Often used to discover and validate underlying theoretical constructs
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Factor Analysis Model}
    \begin{itemize}
        \item \textbf{Common Factors}: Latent variables that influence multiple observed variables \pause
        \item \textbf{Factor Loadings}: Relationships between observed variables and common factors \pause
        \item \textbf{Unique Factors}: Variable-specific variance not explained by common factors \pause
        \item \textbf{Core Assumption}: $X_i = \lambda_{i1}F_1 + \lambda_{i2}F_2 + \cdots + \lambda_{ik}F_k + U_i$
              \begin{itemize}
                  \item $X_i$ = observed variable, $F_j$ = common factors, $U_i$ = unique factor
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Factor Analysis vs. PCA: Key Differences}
    \begin{center}
    \begin{tabular}{|p{5cm}|p{5cm}|}
    \hline
    \textbf{Principal Component Analysis} & \textbf{Factor Analysis} \\
    \hline
    Dimensionality reduction & Latent variable modeling \\
    \hline
    Components are linear combinations of all variables & Factors are hypothetical constructs \\
    \hline
    Explains total variance & Explains common variance only \\
    \hline
    No measurement error model & Explicitly models unique variance \\
    \hline
    Descriptive technique & Statistical model with assumptions \\
    \hline
    \end{tabular}
    \end{center}
\end{frame}

\section{Educational Assessment: Factor Analysis}

\begin{frame}
    \frametitle{Educational Assessment: Factor Analysis}
    \textbf{Reanalyzing our synthetic student data with Factor Analysis}
    \begin{itemize}
        \item \textbf{Same Dataset}: 100 students, 6 variables (MathTest, VerbalTest, SocialSkills, Leadership, RandomVar1, RandomVar2) \pause
        \item \textbf{Known Structure}: Intelligence factor + Personality factor + noise \pause
        \item \textbf{FA Advantage}: Should better identify the true 2-factor structure \pause
        \item \textbf{Comparison Goal}: See how FA handles measurement error vs PCA \pause
        \item \textbf{Scripts}: \texttt{educational\_pca.py} vs \texttt{educational\_fa.py}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Expected Factor Analysis Results}
    \textbf{Anticipated Findings:}
    \begin{itemize}
        \item Factor 1: Intelligence (Math, Verbal tests) \pause
        \item Factor 2: Personality (Social skills, Leadership) \pause
        \item Random variables should show low communalities \pause
    \end{itemize}
\end{frame}

\section{Educational Assessment: PCA vs FA Comparison}

\begin{frame}
    \frametitle{Educational Assessment: Method Comparison}
    \textbf{Comparing PCA and Factor Analysis Results:}
    \begin{itemize}
        \item \textbf{Factor Recovery}: How well does each method identify the true 2-factor structure? \pause
        \item \textbf{Noise Handling}: Which method better separates meaningful variables from random noise? \pause
        \item \textbf{Interpretability}: Are the PCA components or FA factors more interpretable? \pause
        \item \textbf{Variance Explained}: Total variance (PCA) vs common variance (FA) \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Assessment: Key Insights}
    \textbf{What We Learn from This Comparison:}
    \begin{itemize}
        \item \textbf{Known Ground Truth}: Synthetic data allows validation of both methods \pause
        \item \textbf{Method Strengths}: PCA maximizes variance, FA models latent constructs \pause
        \item \textbf{Practical Choice}: Depends on whether you want data reduction or construct modeling \pause
    \end{itemize}
\end{frame}

% ============================================================================
% PART III: EXAMPLE 2 - EUROPEAN STOCK MARKETS
% ============================================================================

\part{Example 2: European Stock Markets}

\begin{frame}
    \partpage
\end{frame}

\section{European Stock Markets: PCA Analysis}

\begin{frame}
    \frametitle{European Stock Markets: PCA Analysis}
    This section demonstrates PCA applied to financial markets using synthetic European stock market data.
    \begin{itemize}
        \item \textbf{Dataset}: 4 major European indices (DAX, SMI, CAC, FTSE) over 1,860 trading days. \pause
        \item \textbf{Research Question}: How integrated are European financial markets? Can we identify common market factors? \pause
        \item \textbf{Method}: Standardized PCA on correlation matrix of daily returns. \pause
    \end{itemize}
    \vspace{6pt}
    Scripts: \texttt{invest\_pca.py} (PCA) | \texttt{invest\_fa.py} (Factor Analysis)
\end{frame}

\begin{frame}
    \frametitle{Dataset: European Market Indices}
    \begin{itemize}
        \item \textbf{DAX (Germany)}: Frankfurt Stock Exchange — largest European economy \pause
        \item \textbf{SMI (Switzerland)}: Swiss Market Index — major financial center \pause
        \item \textbf{CAC (France)}: Paris Stock Exchange — core eurozone market \pause
        \item \textbf{FTSE (UK)}: London Stock Exchange — major international hub \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Market Integration}
    When running the analysis, we observe:
    \begin{itemize}
        \item \textbf{PC1}: Explains $\sim$97\% of total variance \pause
              \begin{itemize}
                  \item Eigenvalue $\approx$ 3.9 (well above Kaiser threshold of 1.0) \pause
                  \item Represents a \textit{common European market factor} \pause
                  \item All markets load positively — they move together \pause
              \end{itemize}
        \item \textbf{PC2-PC4}: Explain only $\sim$3\% combined variance \pause
              \begin{itemize}
                  \item Capture market-specific idiosyncrasies \pause
                  \item Currency effects, country-specific political events \pause
                  \item Largely noise for portfolio construction \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\section{European Stock Markets: Factor Analysis}

\begin{frame}
    \frametitle{European Stock Markets: Factor Analysis}
    Applying Factor Analysis to the same financial markets data:
    \begin{itemize}
        \item \textbf{Same Dataset}: 4 major European indices (DAX, SMI, CAC, FTSE) over 1,860 trading days \pause
        \item \textbf{FA Approach}: Model latent market factors affecting stock returns \pause
        \item \textbf{Key Difference}: FA focuses on common market factors vs PCA's variance maximization \pause
        \item \textbf{Expected Factors}: Common European market factor, country-specific factors \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{invest\_fa.py}
\end{frame}

\begin{frame}
    \frametitle{European Markets FA: Expected Results}
    \textbf{Factor Analysis Expectations:}
    \begin{itemize}
        \item \textbf{Factor 1}: Common European market factor (systematic risk) \pause
        \item \textbf{Remaining Factors}: Country-specific or currency effects \pause
        \item \textbf{High Communalities}: Most variance should be common (shared) \pause
        \item \textbf{Model Fit}: Should show excellent fit given high correlations \pause
    \end{itemize}
\end{frame}

\section{European Stock Markets: PCA vs FA Comparison}

\begin{frame}
    \frametitle{European Markets: Method Comparison}
    \textbf{Comparing PCA and Factor Analysis Results:}
    \begin{itemize}
        \item \textbf{Dominant Factor}: Both methods should identify strong first component/factor \pause
        \item \textbf{Variance Concentration}: 97\% in PC1 vs high communalities in Factor 1 \pause
        \item \textbf{Interpretation}: Market integration story consistent across methods \pause
        \item \textbf{Practical Use}: Both support single-factor portfolio risk models \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{European Markets: Key Insights}
    \textbf{What We Learn from This Comparison:}
    \begin{itemize}
        \item \textbf{High Correlation Case}: When variables are highly correlated, methods converge \pause
        \item \textbf{Single Factor Dominance}: Both methods identify market integration \pause
        \item \textbf{Practical Equivalence}: For portfolio management, results are very similar \pause
    \end{itemize}
\end{frame}

% ============================================================================
% PART IV: EXAMPLE 3 - KUIPER BELT OBJECTS
% ============================================================================

\part{Example 3: Kuiper Belt Objects}

\begin{frame}
    \partpage
\end{frame}

\section{Kuiper Belt Objects: PCA Analysis}

\begin{frame}
    \frametitle{Kuiper Belt Objects: PCA Analysis}
    This section demonstrates PCA applied to astronomical data from the outer solar system.
    \begin{itemize}
        \item \textbf{Dataset}: Orbital parameters of 98 trans-Neptunian objects (TNOs) and Kuiper Belt objects \pause
        \item \textbf{Research Question}: What are the main modes of orbital variation? Can we identify distinct dynamical populations? \pause
        \item \textbf{Method}: Standardized PCA on 5 orbital elements with different physical units \pause
    \end{itemize}
    \vspace{6pt}
    Scripts: \texttt{kuiper\_pca.py} (PCA) | \texttt{kuiper\_fa.py} (Factor Analysis)
\end{frame}

\begin{frame}
    \frametitle{Dataset: Orbital Parameters}
    Five key orbital elements describe each object's motion:
    \begin{itemize}
        \item \textbf{a} (AU): Semi-major axis — average distance from Sun (30-150 AU) \pause
        \item \textbf{e}: Eccentricity — orbital shape (0=circle, 1=parabola) \pause
        \item \textbf{i} (degrees): Inclination — tilt relative to solar system plane \pause
        \item \textbf{H} (magnitude): Absolute magnitude — brightness/size indicator \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Orbital Excitation}
    When running the analysis, we observe a more balanced variance distribution:
    \begin{itemize}
        \item \textbf{PC1} (36.5\% variance): Orbital excitation dimension \pause
              \begin{itemize}
                  \item Correlates semi-major axis, eccentricity, and inclination \pause
                  \item Separates dynamically "hot" (excited) from "cold" (pristine) populations \pause
              \end{itemize}
        \item \textbf{PC2} (23.2\% variance): Size-distance relationship \pause
              \begin{itemize}
                  \item May reflect observational bias or physical size distribution \pause
              \end{itemize}
        \item \textbf{PC3} (17.8\% variance): Additional orbital structure \pause
              \begin{itemize}
                  \item First 3 components explain $\sim$77.5\% of variance \pause
                  \item More complex structure than financial markets example \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\section{Kuiper Belt Objects: Factor Analysis}

\begin{frame}
    \frametitle{Kuiper Belt Objects: Factor Analysis}
    Applying Factor Analysis to astronomical orbital dynamics data:
    \begin{itemize}
        \item \textbf{Same Dataset}: 98 trans-Neptunian objects with 5 orbital parameters \pause
        \item \textbf{FA Approach}: Model latent dynamical factors affecting orbital elements \pause
        \item \textbf{Key Difference}: FA focuses on common dynamical processes vs PCA's variance maximization \pause
        \item \textbf{Expected Factors}: Dynamical excitation, size-distance relationships, resonance effects \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{kuiper\_fa.py}
\end{frame}

\begin{frame}
    \frametitle{Kuiper Belt FA: Factor Extraction Results}
    \textbf{Principal Axis Factoring Results:}
    \begin{itemize}
        \item \textbf{Kaiser Criterion}: Suggests 3 factors (eigenvalues > 1.0) \pause
        \item \textbf{Factor 1}: Orbital excitation (high loadings on a, e, i) \pause
              \begin{itemize}
                  \item Represents dynamical "heating" of orbits over solar system history \pause
              \end{itemize}
        \item \textbf{Factor 2}: Object designation effects (data artifact) \pause
        \item \textbf{Factor 3}: Size factor (high loading on absolute magnitude H) \pause
              \begin{itemize}
                  \item Separates size-related observational effects \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\section{Kuiper Belt Objects: PCA vs FA Comparison}

\begin{frame}
    \frametitle{Kuiper Belt: Method Comparison}
    \textbf{Comparing PCA and Factor Analysis Results:}
    \begin{itemize}
        \item \textbf{Multiple Factors}: Both methods identify 2-3 meaningful dimensions \pause
        \item \textbf{Factor Interpretation}: Similar orbital excitation and size factors \pause
        \item \textbf{Variance Distribution}: More balanced across components/factors than financial data \pause
        \item \textbf{Scientific Validity}: Both support established astronomical theories \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kuiper Belt: Key Insights}
    \textbf{What We Learn from This Comparison:}
    \begin{itemize}
        \item \textbf{Complex Structure}: Multiple meaningful factors require both methods \pause
        \item \textbf{Scientific Interpretation}: FA factors may be more physically meaningful \pause
        \item \textbf{Model Validation}: FA provides additional fit statistics and diagnostics \pause
    \end{itemize}
\end{frame}

% ============================================================================
% PART V: EXAMPLE 4 - HOSPITAL HEALTH OUTCOMES
% ============================================================================

\part{Example 4: Hospital Health Outcomes}

\begin{frame}
    \partpage
\end{frame}

\section{Hospital Health Outcomes: PCA Analysis}

\begin{frame}
    \frametitle{Hospital Health Outcomes: PCA Analysis}
    This section demonstrates PCA applied to healthcare quality data from US hospitals.
    \begin{itemize}
        \item \textbf{Dataset}: Health outcome metrics for 50 US hospitals across 8 performance indicators \pause
        \item \textbf{Research Question}: What are the main dimensions of hospital quality? Can we rank hospital performance? \pause
        \item \textbf{Method}: Standardized PCA on healthcare metrics with different units and scales \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{hospitals\_pca.py}
\end{frame}

\begin{frame}
    \frametitle{Dataset: Hospital Performance Metrics}
    Eight key hospital quality indicators (with desired direction):
    \begin{itemize}
        \item \textbf{MortalityRate} (\%): Hospital mortality rate (lower $\rightarrow$ better) \pause
        \item \textbf{ReadmissionRate} (\%): 30-day readmission rate (lower $\rightarrow$ better) \pause
        \item \textbf{PatientSatisfaction} (0-100): Patient satisfaction score (higher $\rightarrow$ better) \pause
        \item \textbf{AvgLengthStay} (days): Average length of stay (shorter $\rightarrow$ better) \pause
        \item \textbf{InfectionRate} (\%): Hospital-acquired infections (lower $\rightarrow$ better) \pause
        \item \textbf{NurseRatio}: Nurse-to-patient ratio (higher $\rightarrow$ better) \pause
        \item \textbf{SurgicalComplications} (\%): Surgical complication rate (lower $\rightarrow$ better) \pause
        \item \textbf{EDWaitTime} (minutes): Emergency dept. wait time (lower $\rightarrow$ better) \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Strong Quality Factor}
    When running the analysis, we observe strong factor concentration:
    \begin{itemize}
        \item \textbf{PC1} (70.5\% variance): Overall hospital quality dimension \pause
              \begin{itemize}
                  \item Eigenvalue $\approx$ 5.6 (well above Kaiser threshold) \pause
                  \item High loadings across all quality metrics \pause
                  \item Represents systematic organizational excellence \pause
              \end{itemize}
        \item \textbf{PC2} (8.5\% variance): Efficiency vs. thoroughness trade-off \pause
              \begin{itemize}
                  \item May separate length of stay patterns \pause
                  \item Different care philosophies or patient populations \pause
              \end{itemize}
        \item \textbf{PC1 dominance}: Similar to financial markets but for quality reasons \pause
    \end{itemize}
\end{frame}

\section{Hospital Health Outcomes: Factor Analysis}

\begin{frame}
    \frametitle{Hospital Health Outcomes: Factor Analysis}
    Applying Factor Analysis to healthcare quality data:
    \begin{itemize}
        \item \textbf{Same Dataset}: Health outcome metrics for 50 US hospitals across 8 performance indicators \pause
        \item \textbf{FA Approach}: Model latent quality factors affecting hospital performance \pause
        \item \textbf{Key Difference}: FA focuses on common quality factors vs PCA's variance maximization \pause
        \item \textbf{Expected Factors}: Overall quality factor, efficiency vs thoroughness trade-offs \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{hospitals\_fa.py}
\end{frame}

\begin{frame}
    \frametitle{Hospital FA: Expected Results}
    \textbf{Factor Analysis Expectations:}
    \begin{itemize}
        \item \textbf{Factor 1}: Overall hospital quality (organizational excellence) \pause
        \item \textbf{Factor 2}: Efficiency vs thoroughness trade-offs \pause
        \item \textbf{High Communalities}: Quality metrics should be highly correlated \pause
        \item \textbf{Model Fit}: Excellent fit expected due to quality halo effect \pause
    \end{itemize}
\end{frame}

\section{Hospital Health Outcomes: PCA vs FA Comparison}

\begin{frame}
    \frametitle{Hospital Outcomes: Method Comparison}
    \textbf{Comparing PCA and Factor Analysis Results:}
    \begin{itemize}
        \item \textbf{Dominant Factor}: Both methods should identify strong quality factor \pause
        \item \textbf{Variance Concentration}: 70\% in PC1 vs high communalities \pause
        \item \textbf{Quality Rankings}: Hospital rankings should be very similar \pause
        \item \textbf{Policy Applications}: Both support composite quality measures \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Hospital Outcomes: Key Insights}
    \textbf{What We Learn from This Comparison:}
    \begin{itemize}
        \item \textbf{Quality Halo Effect}: Strong correlations make methods converge \pause
        \item \textbf{Practical Equivalence}: For hospital rankings, both methods work well \pause
        \item \textbf{Policy Choice}: Depends on whether you want data reduction or quality modeling \pause
    \end{itemize}
\end{frame}

% ============================================================================
% PART VI: OVERALL COMPARISON AND GUIDELINES
% ============================================================================

\part{Overall Comparison and Guidelines}

\begin{frame}
    \partpage
\end{frame}

\section{Overall Method Comparison}

\begin{frame}
    \frametitle{Summary of Method Comparisons Across Examples}
    \textbf{What We Learned from Four Different Domains:}
    \begin{itemize}
        \item \textbf{Educational Assessment}: FA better recovers known factor structure \pause
        \item \textbf{European Stock Markets}: Methods converge when correlations are very high \pause
        \item \textbf{Kuiper Belt Objects}: FA provides better scientific interpretability \pause
        \item \textbf{Hospital Quality}: Methods give similar results for practical applications \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Method Comparison Criteria}
    \textbf{Evaluation Framework:}
    \begin{itemize}
        \item Variance explained vs factor interpretability
        \item Treatment of measurement error and unique variance
        \item Practical applications and domain fit
    \end{itemize}
\end{frame}

\section{Guidelines for Method Selection}

\begin{frame}
    \frametitle{When to Use PCA vs Factor Analysis}
    \textbf{Use PCA when:}
    \begin{itemize}
        \item Primary goal is dimensionality reduction \pause
        \item You want to maximize variance explained \pause
        \item Data compression or noise reduction is the objective \pause
        \item You don't have strong theoretical expectations about latent constructs \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{When to Use Factor Analysis}
    \textbf{Use Factor Analysis when:}
    \begin{itemize}
        \item You want to model latent constructs or theoretical factors \pause
        \item Understanding measurement error and unique variance is important \pause
        \item You need factor rotation for cleaner interpretation \pause
        \item Confirmatory analysis of hypothesized factor structures \pause
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Recommendations}
    \begin{itemize}
        \item \textbf{Start with EDA}: Use PCA first to understand your data structure \pause
        \item \textbf{Theory-driven analysis}: Apply Factor Analysis when you have theoretical expectations \pause
        \item \textbf{Compare both methods}: Results convergence strengthens conclusions \pause
        \item \textbf{Consider sample size}: Factor Analysis requires larger samples \pause
        \item \textbf{Validate results}: Use cross-validation, external criteria, or confirmatory approaches
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{A Word of Caution}
    \begin{itemize}
        \item Neither method can \textit{prove} a factor structure is correct \pause
        \item Multiple equally valid models may exist for the same dataset \pause
        \item Always combine statistical results with domain knowledge
    \end{itemize}
\end{frame}
\end{document}