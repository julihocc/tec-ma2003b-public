\documentclass[aspectratio=169]{beamer}

% Presentation metadata
\title{Factor Analysis}
\author{Dr. Juliho Castillo}
\institute{Tecnológico de Monterrey}
\date{\today}

% Additional packages for the presentation
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}

% Use Metropolis theme with a blue color palette
% (Metropolis should be available in the TeX distribution; if not, install the MTheme package.)
\usetheme{metropolis}
% Metro options: tidy title and a subtle progress indicator
\metroset{titleformat=smallcaps,progressbar=frametitle}


\begin{document}

% Title slide
\begin{frame}
    \titlepage
\end{frame}

% Table of contents
\begin{frame}
    \tableofcontents
\end{frame}

% ============================================================================
% PART I: PRINCIPAL COMPONENT ANALYSIS
% ============================================================================

\part{Principal Component Analysis}

\begin{frame}
    \partpage
\end{frame}

\section{Introduction to Multivariate Analysis}

\begin{frame}[fragile]
    \frametitle{Multivariate Analysis Overview}
    \begin{itemize}
        \item \textbf{Multivariate Analysis}: Statistical methods for analyzing multiple variables simultaneously \pause
        \item \textbf{Key Challenge}: Understanding relationships among many correlated variables \pause
        \item \textbf{Two Main Approaches}: 
              \begin{itemize}
                  \item \textit{Principal Component Analysis (PCA)}: Dimensionality reduction technique \pause
                  \item \textit{Factor Analysis}: Latent variable modeling technique \pause
              \end{itemize}
        \item \textbf{This Course}: We'll explore both methods using the same datasets for direct comparison
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Structure Overview}
    \textbf{Part I: Principal Component Analysis}
    \begin{itemize}
        \item PCA theory and mathematical foundation
        \item Four comprehensive examples across different domains
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Structure: Factor Analysis}
    \textbf{Part II: Factor Analysis}
    \begin{itemize}
        \item Factor Analysis theory and modeling approach  
        \item Same datasets analyzed with Factor Analysis
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Structure: Comparison}
    \textbf{Part III: Comparison and Applications}
    \begin{itemize}
        \item Side-by-side comparison of results
        \item Guidelines for choosing the appropriate method
    \end{itemize}
\end{frame}

\section{Principal Component Analysis}

\begin{frame}
    \frametitle{Refresher: What is PCA?}
    \begin{itemize}
        \item Principal Component Analysis (PCA) is a linear method for \textbf{dimension reduction}. \pause
        \item It finds orthogonal directions (principal components) that capture the largest possible variance in the data. \pause
        \item PCA produces new variables (components) that are linear combinations of the original observed variables. \pause
        \item Use cases: visualization, noise reduction, pre-processing before supervised learning, and exploratory data analysis. \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Mathematical formulation}
    Let $\mathbf{x}\in\mathbb{R}^p$ be a random vector with mean $\mu$ and covariance matrix $\Sigma$. After centering the data ($\mathbf{x}-\mu$):
    \begin{itemize}
        \item Find eigenvalues $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p$ and orthonormal eigenvectors $\mathbf{v}_1,\dots,\mathbf{v}_p$ of $\Sigma$: $\Sigma\mathbf{v}_j=\lambda_j\mathbf{v}_j$. \pause
        \item The $j$-th principal component is $z_j=\mathbf{v}_j^{\top}(\mathbf{x}-\mu)$. \pause
        \item Variance explained by component $j$ is $\mathrm{Var}(z_j)=\lambda_j$. The proportion explained is $\lambda_j/\sum_{k=1}^p\lambda_k$. \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Computation steps (practical)}
    \begin{enumerate}
        \item Standardize variables if they are on different scales (use correlation matrix) or center only if scales are comparable (use covariance matrix). \pause
        \item Compute covariance (or correlation) matrix $S$ from the data. \pause
        \item Compute eigen decomposition $S=V\Lambda V^{\top}$. \pause
        \item Form principal component scores: $Z = X_c V$ (where $X_c$ is centered data and columns of $V$ are eigenvectors). \pause
        \item Inspect eigenvalues, cumulative variance, and scree plot to decide how many components to keep. \pause
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Deciding how many components to retain}
    Common heuristics and formal approaches:
    \begin{itemize}
        \item Kaiser criterion: keep components with eigenvalue $>1$ (applies when using correlation matrix). \pause
        \item Cumulative variance: keep the smallest number of components that explain a target (e.g., 70--90\%) of total variance. \pause
        \item Scree plot: look for the "elbow" where additional components contribute little incremental variance. \pause
        \item Parallel analysis: compare empirical eigenvalues to those obtained from random data — keep components with larger eigenvalues than random. \pause
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{PCA vs Factor Analysis (reminder)}
    \begin{itemize}
        \item PCA: descriptive linear combinations that maximize variance; components are exact linear functions of observed variables and need not have a causal or measurement model interpretation. \pause
        \item Factor Analysis: a statistical model that explicitly decomposes observed variance into common (shared) variance explained by latent factors and unique variance (errors). \pause
        \item Practical rule: use PCA for dimension reduction and data compression; use Factor Analysis when your goal is to model latent constructs and separate common from unique variance. \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical tips and pitfalls}
    \begin{itemize}
        \item Always check variable scales; standardize when necessary. \pause
        \item PCA is sensitive to outliers — inspect data and consider robust alternatives if needed. \pause
        \item Interpret components via loadings (eigenvectors) and by examining which variables contribute strongly to each component. \pause
        \item Rotation is not standard in PCA (rotation reassigns variance) — if interpretability is a priority, consider Factor Analysis with rotation. \pause
        \item When reporting, include: eigenvalues table, proportion of variance, cumulative variance, scree plot, and a table of loadings (component matrix). \pause
    \end{itemize}
\end{frame}

\section{Educational Assessment: Synthetic PCA Example}

\begin{frame}
    \frametitle{Educational Assessment: Synthetic PCA Example}
    This section demonstrates PCA using controlled synthetic data with known factor structure to validate the method and teach key concepts.
    \begin{itemize}
        \item \textbf{Dataset}: Student assessment data with 6 variables (100 students) \pause
        \item \textbf{Research Question}: Can PCA recover the underlying ability factors? How does it separate meaningful structure from noise? \pause
        \item \textbf{Method}: Standardized PCA on synthetic data with known latent factors \pause
    \end{itemize}
    \vspace{6pt}
    Scripts: \texttt{educational\_pca.py} (PCA) | \texttt{educational\_fa.py} (FA comparison)
\end{frame}

\begin{frame}
    \frametitle{Dataset: Student Assessment Variables}
    Six variables representing different aspects of student ability:
    \begin{itemize}
        \item \textbf{MathTest}: Mathematics assessment score \pause
        \item \textbf{VerbalTest}: Verbal reasoning assessment score \pause
        \item \textbf{SocialSkills}: Social competency rating \pause
        \item \textbf{Leadership}: Leadership ability rating \pause
        \item \textbf{RandomVar1, RandomVar2}: Pure noise controls \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Known Factor Structure}
    \textbf{Ground Truth for Validation:}
    \begin{itemize}
        \item \textit{Intelligence Factor}: Affects MathTest (0.85 loading) and VerbalTest (0.80 loading) \pause
        \item \textit{Personality Factor}: Affects SocialSkills (0.85 loading) and Leadership (0.80 loading) \pause
        \item Measurement error added to all meaningful variables (0.2-0.25 noise levels) \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Context: Pedagogical Value}
    \textbf{Why This Example Works for Learning:}
    \begin{itemize}
        \item Ground truth known $\rightarrow$ can validate PCA's ability to recover factors \pause
        \item Realistic psychological assessment scenario that students understand \pause
        \item Clear separation between meaningful variables and pure noise \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Context: Learning Outcomes}
    \textbf{Key Concepts Students Will Master:}
    \begin{itemize}
        \item Understand how PCA handles correlated variables driven by latent factors \pause
        \item See how noise components are separated from meaningful structure \pause
        \item Learn to interpret component loadings in context of known relationships \pause
        \item Practice using scree plots and eigenvalue criteria for component selection \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Factor Recovery}
    When running the analysis, we observe meaningful factor separation:
    \begin{itemize}
        \item \textbf{PC1} (36.7\% variance): General ability factor \pause
              \begin{itemize}
                  \item Eigenvalue $\approx$ 2.2 (well above Kaiser threshold) \pause
                  \item Captures common variance across all meaningful measures \pause
                  \item Reflects "halo effect" common in ability assessments \pause
              \end{itemize}
        \item \textbf{PC2} (30.8\% variance): Specific ability dimensions \pause
              \begin{itemize}
                  \item May separate cognitive from social abilities \pause
                  \item Shows how PCA can capture multiple meaningful factors \pause
              \end{itemize}
        \item \textbf{PC3-PC4} (30.2\% variance): Additional structure and measurement error \pause
        \item \textbf{PC5-PC6} (2.3\% variance): Pure noise components \pause
              \begin{itemize}
                  \item Very low eigenvalues ($<$ 0.15) clearly identify noise floor \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Interpretation: Ability Structure}
    \textbf{PC1 as General Ability:}
    \begin{itemize}
        \item Strong loadings on all meaningful variables (MathTest, VerbalTest, SocialSkills, Leadership) \pause
        \item Weak loadings on noise variables (RandomVar1, RandomVar2) \pause
        \item Demonstrates how PCA identifies common underlying factors \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Interpretation: Validation Success}
    \textbf{Method Validation Results:}
    \begin{itemize}
        \item \textit{Factor Recovery}: Meaningful variables show loading strength $\sim$0.45-0.50 \pause
        \item \textit{Noise Separation}: Random variables show much weaker loadings $<$0.33 \pause
        \item \textit{Structure Detection}: Clear eigenvalue drop separates signal from noise \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scree Plot: Learning Component Selection}
    The scree plot demonstrates key decision-making concepts:
    \begin{itemize}
        \item Eigenvalues decline from meaningful structure ($>$0.8) to pure noise ($<$0.15) \pause
        \item Kaiser criterion suggests retaining 4 components ($\lambda > 1$) \pause
        \item 67\% variance explained by first 2 components is realistic for assessment data \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scree Plot: Teaching Points}
    \textbf{Key Learning Concepts:}
    \begin{itemize}
        \item No single "correct" number of components — depends on purpose \pause
        \item Dramatic eigenvalue drop after PC4 indicates transition to noise \pause
        \item Real data rarely shows as clean separation as this synthetic example \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Biplot: Students and Abilities}
    The biplot reveals educational assessment patterns:
    \begin{itemize}
        \item \textbf{Variable arrows}: Show how different abilities relate to PC dimensions \pause
              \begin{itemize}
                  \item Academic tests (Math, Verbal) cluster together if similar factor loadings \pause
                  \item Social measures (Skills, Leadership) may point in different direction \pause
                  \item Random variables show smaller, more scattered loadings \pause
              \end{itemize}
        \item \textbf{Student points}: Individual students positioned in ability space \pause
              \begin{itemize}
                  \item Spread along PC1 shows general ability differences \pause
                  \item Students with high PC1 scores excel across multiple domains \pause
                  \item Can identify students with specific ability profiles \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Applications: Assessment Development}
    \textbf{Assessment Development:}
    \begin{itemize}
        \item Validate that tests measure intended constructs (factor loadings) \pause
        \item Identify redundant measures that capture similar abilities \pause
        \item Design balanced assessments across multiple ability dimensions \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Applications: Student Analysis}
    \textbf{Student Analysis:}
    \begin{itemize}
        \item Create composite ability scores using PC1 weights \pause
        \item Identify students with unusual ability profiles (outliers in PC space) \pause
        \item Track ability development over time using consistent PC scoring \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Educational Applications: Research Applications}
    \textbf{Research Applications:}
    \begin{itemize}
        \item Test theories about ability structure and factor relationships \pause
        \item Compare ability patterns across different populations or contexts \pause
        \item Develop more efficient assessment batteries based on factor structure \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Code Example: Synthetic Data Analysis}
    \begin{itemize}
        \item \textbf{Data generation}: Creates realistic educational assessment scenario with known factors \pause
        \item \textbf{Main analysis}: \texttt{educational\_pca.py} performs PCA with detailed pedagogical interpretation \pause
        \item \textbf{Outputs}:
              \begin{itemize}
                  \item Eigenvalues, explained variance ratios, factor recovery validation \pause
                  \item \texttt{pca\_scree.png}: Scree plot showing signal-to-noise transition \pause
                  \item \texttt{pca\_biplot.png}: Students and abilities in PC space \pause
                  \item Student rankings by PC scores for practical interpretation \pause
              \end{itemize}
        \item \textbf{Usage}: \texttt{cd code/pca\_example \&\& python educational\_pca.py} \pause
        \item \textbf{Factor Analysis}: \texttt{educational\_fa.py} provides direct PCA vs FA comparison \pause
    \end{itemize}
    \vspace{6pt}
    This example is ideal for learning PCA concepts because the ground truth is known, making it easy to validate whether the method works as expected.
\end{frame}

\section{Investment allocation example}

\begin{frame}
    \frametitle{European Stock Markets: PCA Analysis}
    This section demonstrates PCA applied to financial markets using synthetic European stock market data.
    \begin{itemize}
        \item \textbf{Dataset}: 4 major European indices (DAX, SMI, CAC, FTSE) over 1,860 trading days. \pause
        \item \textbf{Research Question}: How integrated are European financial markets? Can we identify common market factors? \pause
        \item \textbf{Method}: Standardized PCA on correlation matrix of daily returns. \pause
    \end{itemize}
    \vspace{6pt}
    Scripts: \texttt{invest\_pca.py} (PCA) | \texttt{invest\_fa.py} (Factor Analysis)
\end{frame}

\begin{frame}
    \frametitle{Dataset: European Market Indices}
    \begin{itemize}
        \item \textbf{DAX (Germany)}: Frankfurt Stock Exchange — largest European economy \pause
        \item \textbf{SMI (Switzerland)}: Swiss Market Index — major financial center \pause
        \item \textbf{CAC (France)}: Paris Stock Exchange — core eurozone market \pause
        \item \textbf{FTSE (UK)}: London Stock Exchange — major international hub \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Financial Data Context}
    \textbf{Analysis Setup:}
    \begin{itemize}
        \item Daily returns standardized (mean=0, std=1) for scale-invariant analysis \pause
        \item Captures correlation structure across different currencies and economies \pause
        \item Reflects market integration through EU membership and globalization \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Market Integration}
    When running the analysis, we observe:
    \begin{itemize}
        \item \textbf{PC1}: Explains $\sim$97\% of total variance \pause
              \begin{itemize}
                  \item Eigenvalue $\approx$ 3.9 (well above Kaiser threshold of 1.0) \pause
                  \item Represents a \textit{common European market factor} \pause
                  \item All markets load positively — they move together \pause
              \end{itemize}
        \item \textbf{PC2-PC4}: Explain only $\sim$3\% combined variance \pause
              \begin{itemize}
                  \item Capture market-specific idiosyncrasies \pause
                  \item Currency effects, country-specific political events \pause
                  \item Largely noise for portfolio construction \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Financial Interpretation: Market Factor}
    \textbf{PC1 as Systematic Risk:}
    \begin{itemize}
        \item Captures \textit{systematic risk} — movements common to all markets \pause
        \item Driven by: EU-wide economic conditions, global financial sentiment, central bank policies \pause
        \item High loadings on all indices $\rightarrow$ European markets are highly integrated \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Portfolio Management Implications}
    \textbf{Key Insights for Investors:}
    \begin{itemize}
        \item \textit{Limited diversification benefit} from spreading across European markets alone \pause
        \item Most portfolio variance comes from exposure to the common factor \pause
        \item Geographic diversification requires markets with different factor exposures \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Visualization: Scree Plot Analysis}
    The scree plot shows a dramatic \textit{``cliff''} pattern:
    \begin{itemize}
        \item Sharp drop from PC1 ($\lambda \approx 3.9$) to PC2 ($\lambda \approx 0.09$) \pause
        \item Clear ``elbow'' indicates one dominant factor \pause
        \item Remaining components are essentially flat (noise floor) \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Component Selection Decision}
    \textbf{Decision Rules Applied:}
    \begin{itemize}
        \item Kaiser criterion: Retain PC1 only ($\lambda > 1$) \pause
        \item Variance threshold: PC1 alone exceeds any reasonable cutoff (80\%, 90\%, 95\%) \pause
        \item Practical conclusion: European markets can be summarized by a single factor \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Biplot: Markets and Time Periods}
    The biplot reveals the factor structure:
    \begin{itemize}
        \item \textbf{Variable arrows} (red): Show market loadings on PC1-PC2 \pause
              \begin{itemize}
                  \item All arrows point in similar direction $\rightarrow$ positive correlation \pause
                  \item Arrow length reflects contribution to variance \pause
                  \item Angle between arrows shows correlation strength \pause
              \end{itemize}
        \item \textbf{Observation points}: Individual trading days in PC space \pause
              \begin{itemize}
                  \item Horizontal spread (PC1): Common market movements \pause
                  \item Vertical spread (PC2): Minor market-specific deviations \pause
                  \item Outliers may represent crisis periods or major events \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical Applications: Risk Management}
    \textbf{Risk Management:}
    \begin{itemize}
        \item Use PC1 scores as a single \textit{European market risk factor} \pause
        \item Portfolio $\beta$ to PC1 determines systematic risk exposure \pause
        \item Stress testing: model extreme PC1 movements \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical Applications: Portfolio Construction}
    \textbf{Portfolio Construction:}
    \begin{itemize}
        \item Market-neutral strategies require offsetting PC1 exposure \pause
        \item Alpha generation focuses on PC2-PC4 (idiosyncratic components) \pause
        \item Diversification requires assets uncorrelated with European factor \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical Applications: Performance Attribution}
    \textbf{Performance Attribution:}
    \begin{itemize}
        \item Decompose returns into market factor (PC1) + specific factors (PC2+) \pause
        \item Distinguish skill from market timing \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Code Example: Running the Analysis}
    \begin{itemize}
        \item \textbf{Data preparation}: \texttt{fetch\_invest.py} generates synthetic European market data \pause
        \item \textbf{Main analysis}: \texttt{invest\_pca.py} performs PCA with detailed financial interpretation \pause
        \item \textbf{Outputs}: 
              \begin{itemize}
                  \item Eigenvalues, explained variance ratios, cumulative variance \pause
                  \item \texttt{invest\_scree.png}: Scree plot for component selection \pause
                  \item \texttt{invest\_biplot.png}: Biplot of markets and time periods \pause
              \end{itemize}
        \item \textbf{Usage}: \texttt{cd code/invest\_example \&\& python invest\_pca.py} \pause
        \item \textbf{Factor Analysis}: \texttt{invest\_fa.py} analyzes European market factors \pause
    \end{itemize}
    \vspace{6pt}
    The script includes detailed py-percent comments for interactive exploration and financial interpretation of all results.
\end{frame}

\section{Kuiper Belt Objects: Astronomical PCA}

\begin{frame}
    \frametitle{Kuiper Belt Objects: PCA Analysis}
    This section demonstrates PCA applied to astronomical data from the outer solar system.
    \begin{itemize}
        \item \textbf{Dataset}: Orbital parameters of 98 trans-Neptunian objects (TNOs) and Kuiper Belt objects \pause
        \item \textbf{Research Question}: What are the main modes of orbital variation? Can we identify distinct dynamical populations? \pause
        \item \textbf{Method}: Standardized PCA on 5 orbital elements with different physical units \pause
    \end{itemize}
    \vspace{6pt}
    Scripts: \texttt{kuiper\_pca.py} (PCA) | \texttt{kuiper\_fa.py} (Factor Analysis)
\end{frame}

\begin{frame}
    \frametitle{Dataset: Orbital Parameters}
    Five key orbital elements describe each object's motion:
    \begin{itemize}
        \item \textbf{a} (AU): Semi-major axis — average distance from Sun (30-150 AU) \pause
        \item \textbf{e}: Eccentricity — orbital shape (0=circle, 1=parabola) \pause
        \item \textbf{i} (degrees): Inclination — tilt relative to solar system plane \pause
        \item \textbf{H} (magnitude): Absolute magnitude — brightness/size indicator \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Dataset: Physical Context}
    \textbf{Physical Context:}
    \begin{itemize}
        \item Objects beyond Neptune's orbit ($\sim$30 AU) in the outer solar system \pause
        \item Different units require standardization for meaningful PCA \pause
        \item Orbital correlations reflect gravitational interactions and formation history \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Dynamical Populations in the Kuiper Belt}
    Three main populations with distinct orbital signatures:
    \begin{itemize}
        \item \textbf{Classical Kuiper Belt} (60\%): Low eccentricity, low inclination \pause
              \begin{itemize}
                  \item Nearly circular orbits around 39-48 AU \pause
                  \item ``Cold'' population — likely formed in place \pause
              \end{itemize}
        \item \textbf{Scattered Disk Objects} (30\%): High eccentricity, distant \pause
              \begin{itemize}
                  \item $e > 0.3$, semi-major axis $> 50$ AU \pause
                  \item Scattered outward by gravitational encounters with Neptune \pause
              \end{itemize}
        \item \textbf{Resonant Objects} (10\%): Locked in orbital resonances \pause
              \begin{itemize}
                  \item 3:2 resonance at $\sim$39.4 AU (like Pluto) \pause
                  \item Captured during Neptune's outward migration \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Orbital Excitation}
    When running the analysis, we observe a more balanced variance distribution:
    \begin{itemize}
        \item \textbf{PC1} (36.5\% variance): Orbital excitation dimension \pause
              \begin{itemize}
                  \item Correlates semi-major axis, eccentricity, and inclination \pause
                  \item Separates dynamically ``hot'' (excited) from ``cold'' (pristine) populations \pause
              \end{itemize}
        \item \textbf{PC2} (23.2\% variance): Size-distance relationship \pause
              \begin{itemize}
                  \item May reflect observational bias or physical size distribution \pause
              \end{itemize}
        \item \textbf{PC3} (17.8\% variance): Additional orbital structure \pause
              \begin{itemize}
                  \item First 3 components explain $\sim$77.5\% of variance \pause
                  \item More complex structure than financial markets example \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Astronomical Interpretation: Dynamical Evolution}
    \textbf{PC1 as Dynamical Excitation:}
    \begin{itemize}
        \item High loadings on distance (a), eccentricity (e), and inclination (i) \pause
        \item Represents gravitational ``heating'' of orbits over solar system history \pause
        \item Separates pristine objects from those scattered by planetary migration \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Astronomical Interpretation: Astrophysical Implications}
    \textbf{Astrophysical Implications:}
    \begin{itemize}
        \item \textit{Formation models}: PC1 scores distinguish formation mechanisms \pause
        \item \textit{Dynamical families}: Objects with similar PC scores likely share evolutionary history \pause
        \item \textit{Size segregation}: Large objects may preferentially survive in certain orbital regions \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scree Plot: Multiple Components Matter}
    Unlike financial markets, the Kuiper Belt shows more distributed variance:
    \begin{itemize}
        \item Gradual decline rather than sharp cliff — no single dominant factor \pause
        \item First eigenvalue $\sim$1.8, others $\sim$1.2, 0.9, 0.8 (above noise level) \pause
        \item Suggests 2-3 meaningful components for dimensional reduction \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scree Plot: Decision Rules}
    \textbf{Decision Rules:}
    \begin{itemize}
        \item Kaiser criterion: Retain 2 components ($\lambda > 1$) \pause
        \item 80\% variance threshold: Need 3 components (77.5\% with 3) \pause
        \item Physical interpretation: Multiple gravitational processes create complex structure \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Biplot: Objects in Orbital Space}
    The biplot reveals orbital relationships:
    \begin{itemize}
        \item \textbf{Variable arrows}: Orbital parameter loadings \pause
              \begin{itemize}
                  \item Clustered arrows (a, e, i) show correlated excitation \pause
                  \item H (brightness) may point differently — size-orbital coupling \pause
              \end{itemize}
        \item \textbf{Object points}: Individual Kuiper Belt objects in PC space \pause
              \begin{itemize}
                  \item Clustering reveals dynamical families \pause
                  \item Outliers represent unusual objects (highly eccentric, large inclination) \pause
                  \item Can identify candidates for detailed follow-up observations \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scientific Applications: Population Studies}
    \textbf{Population Studies:}
    \begin{itemize}
        \item Classify objects into dynamical families using PC scores \pause
        \item Test formation and migration models against observed distributions \pause
        \item Identify rare populations (detached objects, extreme resonances) \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scientific Applications: Observational Planning}
    \textbf{Observational Planning:}
    \begin{itemize}
        \item Target unusual objects (outliers in PC space) for detailed study \pause
        \item Optimize survey strategies based on population structure \pause
        \item Predict undiscovered populations in unsampled orbital regions \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scientific Applications: Comparative Planetology}
    \textbf{Comparative Planetology:}
    \begin{itemize}
        \item Compare our solar system structure to exoplanetary debris disks \pause
        \item Understand how planetary migration shapes outer system architecture \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Code Example: Astronomical Data Analysis}
    \begin{itemize}
        \item \textbf{Data generation}: \texttt{fetch\_kuiper.py} creates synthetic orbital database \pause
        \item \textbf{Main analysis}: \texttt{kuiper\_pca.py} performs PCA with astronomical interpretation \pause
        \item \textbf{Outputs}:
              \begin{itemize}
                  \item Eigenvalues showing distributed variance structure \pause
                  \item \texttt{kuiper\_scree.png}: Scree plot for component selection \pause
                  \item \texttt{kuiper\_biplot.png}: Objects and orbital parameters in PC space \pause
              \end{itemize}
        \item \textbf{Usage}: \texttt{cd code/kuiper\_example \&\& python kuiper\_pca.py} \pause
        \item \textbf{Factor Analysis}: \texttt{kuiper\_fa.py} applies FA to same orbital data \pause
    \end{itemize}
    \vspace{6pt}
    The script demonstrates PCA applied to scientific data with multiple meaningful components and physical interpretation of mathematical results.
\end{frame}

\section{Hospital Health Outcomes: Healthcare PCA}

\begin{frame}
    \frametitle{Hospital Health Outcomes: PCA Analysis}
    This section demonstrates PCA applied to healthcare quality data from US hospitals.
    \begin{itemize}
        \item \textbf{Dataset}: Health outcome metrics for 50 US hospitals across 8 performance indicators \pause
        \item \textbf{Research Question}: What are the main dimensions of hospital quality? Can we rank hospital performance? \pause
        \item \textbf{Method}: Standardized PCA on healthcare metrics with different units and scales \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{lessons/4\_Factor\_Analysis/code/hospitals\_example/hospitals\_example.py}
\end{frame}

\begin{frame}
    \frametitle{Dataset: Hospital Performance Metrics}
    Eight key hospital quality indicators (with desired direction):
    \begin{itemize}
        \item \textbf{MortalityRate} (\%): Hospital mortality rate (lower $\rightarrow$ better) \pause
        \item \textbf{ReadmissionRate} (\%): 30-day readmission rate (lower $\rightarrow$ better) \pause
        \item \textbf{PatientSatisfaction} (0-100): Patient satisfaction score (higher $\rightarrow$ better) \pause
        \item \textbf{AvgLengthStay} (days): Average length of stay (shorter $\rightarrow$ better) \pause
        \item \textbf{InfectionRate} (\%): Hospital-acquired infections (lower $\rightarrow$ better) \pause
        \item \textbf{NurseRatio}: Nurse-to-patient ratio (higher $\rightarrow$ better) \pause
        \item \textbf{SurgicalComplications} (\%): Surgical complication rate (lower $\rightarrow$ better) \pause
        \item \textbf{EDWaitTime} (minutes): Emergency dept. wait time (lower $\rightarrow$ better) \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Healthcare Context: Quality Measurement}
    \textbf{Clinical Significance:}
    \begin{itemize}
        \item Metrics reflect different aspects of hospital care: safety, effectiveness, patient experience \pause
        \item Used by CMS for Hospital Compare and Value-Based Purchasing programs \pause
        \item Risk-adjusted for patient acuity and case mix differences \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Quality Improvement Context:}
    \begin{itemize}
        \item Different units require standardization for meaningful comparison \pause
        \item Strong correlations expected due to organizational quality culture \pause
        \item "Halo effect": well-managed hospitals perform well across multiple domains \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Typical PCA Results: Strong Quality Factor}
    When running the analysis, we observe strong factor concentration:
    \begin{itemize}
        \item \textbf{PC1} (70.5\% variance): Overall hospital quality dimension \pause
              \begin{itemize}
                  \item Eigenvalue $\approx$ 5.6 (well above Kaiser threshold) \pause
                  \item High loadings across all quality metrics \pause
                  \item Represents systematic organizational excellence \pause
              \end{itemize}
        \item \textbf{PC2} (8.5\% variance): Efficiency vs. thoroughness trade-off \pause
              \begin{itemize}
                  \item May separate length of stay patterns \pause
                  \item Different care philosophies or patient populations \pause
              \end{itemize}
        \item \textbf{PC1 dominance}: Similar to financial markets but for quality reasons \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Healthcare Interpretation: Quality Halo Effect}
    \textbf{PC1 as Organizational Quality:}
    \begin{itemize}
        \item Represents systematic differences in hospital management and culture \pause
        \item High-performing hospitals excel across: mortality, readmissions, infections, satisfaction \pause
        \item Reflects comprehensive quality improvement programs and leadership \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Clinical Implications:}
    \begin{itemize}
        \item \textit{System-wide improvement}: Quality is not isolated to single metrics \pause
        \item \textit{Resource allocation}: Investment in organizational excellence pays dividends across domains \pause
        \item \textit{Best practices}: High PC1 hospitals can serve as models for improvement \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Hospital Quality Rankings}
    PC1 scores enable comprehensive hospital ranking:
    \begin{itemize}
        \item \textbf{Top performers} (high PC1): Low mortality, infections, wait times + high satisfaction, nurse ratios \pause
        \item \textbf{Bottom performers} (low PC1): High mortality, readmissions + low satisfaction, poor staffing \pause
        \textbf{Composite quality score}: Single metric capturing 70\% of quality variation \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Policy Applications:}
    \begin{itemize}
        \item \textit{Public reporting}: PC1-based hospital star ratings \pause
        \item \textit{Value-based purchasing}: Reimbursement tied to PC1 performance \pause
        \textit{Quality improvement}: Target comprehensive organizational change \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Scree Plot: Dominant Quality Factor}
    The scree plot shows strong factor concentration similar to financial markets:
    \begin{itemize}
        \item Sharp drop from PC1 ($\lambda \approx 5.6$) to PC2 ($\lambda \approx 0.7$) \pause
        \item Clear evidence of single dominant quality factor \pause
        \item Remaining components capture minor variations and noise \pause
    \end{itemize}
    \vspace{12pt}
    \textbf{Healthcare Decision Rules:}
    \begin{itemize}
        \item Kaiser criterion: Retain PC1 only for overall quality assessment \pause
        \item 70\% variance: PC1 alone exceeds most reasonable cutoffs \pause
        \item Clinical interpretation: Hospital quality is largely unidimensional \pause
        \item Practical conclusion: Focus quality improvement efforts system-wide \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Biplot: Hospitals in Quality Space}
    The biplot reveals quality structure and hospital positioning:
    \begin{itemize}
        \item \textbf{Variable arrows}: All point in similar direction (quality halo effect) \pause
              \begin{itemize}
                  \item Mortality, infections, complications load negatively (lower = better) \pause
                  \item Satisfaction, nurse ratios load positively (higher = better) \pause
                  \item Arrow clustering confirms correlated quality dimensions \pause
              \end{itemize}
        \item \textbf{Hospital points}: Individual hospitals positioned by quality profile \pause
              \begin{itemize}
                  \item Right side: High-quality hospitals across multiple metrics \pause
                  \item Left side: Hospitals needing comprehensive improvement \pause
                  \item Outliers: Unique performance patterns for investigation \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Healthcare Management Applications}
    \textbf{Quality Assessment \& Benchmarking:}
    \begin{itemize}
        \item Use PC1 scores for comprehensive hospital quality rankings \pause
        \item Identify peer groups with similar quality profiles for comparison \pause
        \item Track quality improvement over time using longitudinal PC1 trends \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Strategic Planning \& Improvement:}
    \begin{itemize}
        \item Target system-wide organizational excellence rather than isolated metrics \pause
        \item Study best practices from high PC1 hospitals for replication \pause
        \item Allocate resources to comprehensive quality programs with broad impact \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Regulatory \& Policy Applications:}
    \begin{itemize}
        \item Inform value-based purchasing and quality incentive programs \pause
        \item Support public reporting initiatives with composite quality measures \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Code Example: Healthcare Quality Analysis}
    \begin{itemize}
        \item \textbf{Data generation}: \texttt{fetch\_hospitals.py} creates realistic hospital quality data \pause
        \item \textbf{Main analysis}: \texttt{hospitals\_example.py} performs PCA with healthcare interpretation \pause
        \item \textbf{Outputs}:
              \begin{itemize}
                  \item Hospital quality rankings based on PC1 scores \pause
                  \item Component loadings showing quality metric relationships \pause
                  \item \texttt{hospitals\_scree.png}: Scree plot showing factor concentration \pause
                  \item \texttt{hospitals\_biplot.png}: Hospitals and metrics in quality space \pause
              \end{itemize}
        \item \textbf{Usage}: \texttt{cd code/hospitals\_example \&\& python hospitals\_example.py} \pause
    \end{itemize}
    \vspace{6pt}
    The script demonstrates PCA applied to healthcare quality assessment with practical applications for hospital management and healthcare policy.
\end{frame}

% ============================================================================
% PART II: FACTOR ANALYSIS
% ============================================================================

\part{Factor Analysis}

\begin{frame}
    \partpage
\end{frame}

\section{Introduction to Factor Analysis}

\begin{frame}[fragile]
    \frametitle{What is Factor Analysis?}
    \begin{itemize}
        \item A statistical method for modeling relationships among \textbf{observed variables} using \textbf{latent factors}. \pause
        \item It uses a smaller number of \textit{unobserved variables}, known as \textbf{common factors}. \pause
        \item \textbf{Key Distinction from PCA}: Explicitly models measurement error and unique variance \pause
        \item Often used to discover and validate underlying theoretical constructs
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Factor Analysis Model}
    \begin{itemize}
        \item \textbf{Common Factors}: Latent variables that influence multiple observed variables \pause
        \item \textbf{Factor Loadings}: Relationships between observed variables and common factors \pause
        \item \textbf{Unique Factors}: Variable-specific variance not explained by common factors \pause
        \item \textbf{Core Assumption}: $X_i = \lambda_{i1}F_1 + \lambda_{i2}F_2 + \cdots + \lambda_{ik}F_k + U_i$
              \begin{itemize}
                  \item $X_i$ = observed variable, $F_j$ = common factors, $U_i$ = unique factor
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Factor Analysis vs. PCA: Key Differences}
    \begin{center}
    \begin{tabular}{|p{5cm}|p{5cm}|}
    \hline
    \textbf{Principal Component Analysis} & \textbf{Factor Analysis} \\
    \hline
    Dimensionality reduction & Latent variable modeling \\
    \hline
    Components are linear combinations of all variables & Factors are hypothetical constructs \\
    \hline
    Explains total variance & Explains common variance only \\
    \hline
    No measurement error model & Explicitly models unique variance \\
    \hline
    Descriptive technique & Statistical model with assumptions \\
    \hline
    \end{tabular}
    \end{center}
    \vspace{12pt}
    \textbf{Next}: We'll analyze the same four datasets with Factor Analysis to see these differences in practice!
\end{frame}

\section{Factor Analysis: Educational Assessment Example}

\begin{frame}
    \frametitle{Educational Assessment: Factor Analysis}
    \textbf{Reanalyzing our synthetic student data with Factor Analysis}
    \begin{itemize}
        \item \textbf{Same Dataset}: 100 students, 6 variables (MathTest, VerbalTest, SocialSkills, Leadership, RandomVar1, RandomVar2) \pause
        \item \textbf{Known Structure}: Intelligence factor + Personality factor + noise \pause
        \item \textbf{FA Advantage}: Should better identify the true 2-factor structure \pause
        \item \textbf{Comparison Goal}: See how FA handles measurement error vs PCA \pause
        \item \textbf{Scripts}: \texttt{educational\_pca.py} vs \texttt{educational\_fa.py}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Expected Factor Analysis Results}
    \textbf{Anticipated Findings:}
    \begin{itemize}
        \item Factor 1: Intelligence (Math, Verbal tests) \pause
        \item Factor 2: Personality (Social skills, Leadership) \pause
        \item Random variables should show low communalities \pause
    \end{itemize}
\end{frame}

\section{Kuiper Belt Objects: Factor Analysis}

\begin{frame}
    \frametitle{Kuiper Belt Objects: Factor Analysis}
    Applying Factor Analysis to astronomical orbital dynamics data:
    \begin{itemize}
        \item \textbf{Same Dataset}: 98 trans-Neptunian objects with 5 orbital parameters \pause
        \item \textbf{FA Approach}: Model latent dynamical factors affecting orbital elements \pause
        \item \textbf{Key Difference}: FA focuses on common dynamical processes vs PCA's variance maximization \pause
        \item \textbf{Expected Factors}: Dynamical excitation, size-distance relationships, resonance effects \pause
    \end{itemize}
    \vspace{6pt}
    Script: \texttt{kuiper\_fa.py}
\end{frame}

\begin{frame}
    \frametitle{Kuiper Belt FA: Factor Assumptions}
    \textbf{Factor Analysis Assumptions for Orbital Data:}
    \begin{itemize}
        \item \textbf{Bartlett's Test}: Tests correlation structure suitability \pause
        \item \textbf{KMO Test}: Measures sampling adequacy for orbital parameters \pause
        \item \textbf{Individual MSA}: Each orbital parameter's factor analysis suitability \pause
        \item \textbf{Result}: Most orbital parameters show acceptable to good MSA values \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kuiper Belt FA: Factor Extraction Results}
    \textbf{Principal Axis Factoring Results:}
    \begin{itemize}
        \item \textbf{Kaiser Criterion}: Suggests 3 factors (eigenvalues > 1.0) \pause
        \item \textbf{Factor 1}: Orbital excitation (high loadings on a, e, i) \pause
              \begin{itemize}
                  \item Represents dynamical "heating" of orbits over solar system history \pause
              \end{itemize}
        \item \textbf{Factor 2}: Object designation effects (data artifact) \pause
        \item \textbf{Factor 3}: Size factor (high loading on absolute magnitude H) \pause
              \begin{itemize}
                  \item Separates size-related observational effects \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kuiper Belt FA: Astronomical Interpretation}
    \textbf{Astrophysical Meaning of Factors:}
    \begin{itemize}
        \item \textbf{Dynamical Excitation Factor}: Captures gravitational scattering effects \pause
              \begin{itemize}
                  \item Objects with high Factor 1 scores: scattered by planetary migration \pause
                  \item Objects with low Factor 1 scores: pristine, formed in-place \pause
              \end{itemize}
        \item \textbf{Size Factor}: Reflects observational selection and physical processes \pause
              \begin{itemize}
                  \item Large objects more easily detected at great distances \pause
                  \item May indicate size-dependent survival mechanisms \pause
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kuiper Belt FA: Model Validation}
    \textbf{Factor Model Quality Assessment:}
    \begin{itemize}
        \item \textbf{RMSR}: Root mean square residuals indicate model fit quality \pause
        \item \textbf{Residual Correlations}: Proportion of large residual correlations \pause
        \item \textbf{Factor Determinacy}: Reliability of factor score estimates \pause
        \item \textbf{Communalities}: How much variance each orbital parameter shares with factors \pause
    \end{itemize}
    \vspace{6pt}
    \textbf{Result}: 3-factor model explains 84\% of common variance in orbital parameters
\end{frame}

% ============================================================================
% PART III: COMPARISON AND APPLICATIONS  
% ============================================================================

\part{Comparison and Applications}

\begin{frame}
    \partpage
\end{frame}

\section{PCA vs Factor Analysis: Direct Comparison}

\begin{frame}
    \frametitle{Method Comparison Overview}
    \textbf{We will compare PCA and Factor Analysis results for:}
    \begin{itemize}
        \item \textbf{Educational Assessment}: Known 2-factor structure with noise
        \item \textbf{European Stock Markets}: High correlation, potential single market factor  
        \item \textbf{Kuiper Belt Objects}: Natural population structure in astronomy
        \item \textbf{Hospital Quality}: Healthcare performance measurement
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Method Comparison Criteria}
    \textbf{Evaluation Framework:}
    \begin{itemize}
        \item Variance explained vs factor interpretability
        \item Treatment of measurement error and unique variance
        \item Practical applications and domain fit
    \end{itemize}
\end{frame}

\section{Guidelines for Method Selection}

\begin{frame}
    \frametitle{When to Use PCA vs Factor Analysis}
    \textbf{Use PCA when:}
    \begin{itemize}
        \item Primary goal is dimensionality reduction \pause
        \item You want to maximize variance explained \pause
        \item Data compression or noise reduction is the objective \pause
        \item You don't have strong theoretical expectations about latent constructs \pause
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{When to Use Factor Analysis}
    \textbf{Use Factor Analysis when:}
    \begin{itemize}
        \item You want to model latent constructs or theoretical factors \pause
        \item Understanding measurement error and unique variance is important \pause
        \item You need factor rotation for cleaner interpretation \pause
        \item Confirmatory analysis of hypothesized factor structures \pause
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Recommendations}
    \begin{itemize}
        \item \textbf{Start with EDA}: Use PCA first to understand your data structure \pause
        \item \textbf{Theory-driven analysis}: Apply Factor Analysis when you have theoretical expectations \pause
        \item \textbf{Compare both methods}: Results convergence strengthens conclusions \pause
        \item \textbf{Consider sample size}: Factor Analysis requires larger samples \pause
        \item \textbf{Validate results}: Use cross-validation, external criteria, or confirmatory approaches
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{A Word of Caution}
    \begin{itemize}
        \item Neither method can \textit{prove} a factor structure is correct \pause
        \item Multiple equally valid models may exist for the same dataset \pause
        \item Always combine statistical results with domain knowledge
    \end{itemize}
\end{frame}
\end{document}
